---
lang: 'ko'
slug: '/E0C308'
---

The platform manages user-contributed knowledge content (text, media, references, etc.) in a highly granular way. We need a content model that treats each piece of knowledge as an atomic unit for easy reuse and linking. Traditional wiki or document systems often treat a whole page or document as the smallest unit, but this system draws inspiration from block-based editors (e.g. Notion, block wikis) where even paragraphs or images are individual units. This **block-based approach** lets us build a graph of knowledge, with rich relationships between small pieces of content.

## Decision

**All content is represented as a "Block" entity in a unified model.** Each Block is a first-class knowledge atom with a unique identifier and type metadata. Text paragraphs, headings, images, files, and even composite documents are all stored as Blocks. Blocks can reference other Blocks or be composed hierarchically (e.g. a document Block containing child Blocks via edges or parent pointers). The blocks are persisted in a single Convex table (e.g. blocks) with a schema like:

- id: Id<"blocks"> – primary key (Convex's object ID).
- type: string – the block type (e.g. "text", "image", "doc", etc.).
- content: any – the content payload. For a text block, this might be a ProseMirror JSON node or plain text; for an image or file block, this could be a reference to file storage ID.
- Metadata fields: e.g. createdBy, createdAt, updatedAt, etc.
- (Optionally) structural fields: e.g. parentId or an order index if we store hierarchy directly. However, hierarchical or semantic relationships are primarily managed by the separate Edges model (see ADR 4).

This decision means **everything that users create or manipulate is a Block**, enabling a uniform API and data handling. A "document" or article is simply a Block of type "doc" that has child Blocks (its paragraphs, images, etc.), which are linked via edges of type "contains" (see ADR 4) or via an array of child IDs in the doc block's content. In MVP, we may implement documents as linear collections of blocks stored in an ordered list, but conceptually they are still blocks containing blocks.

## Rationale

This unified model ensures consistency and flexibility. By treating all content uniformly, we can implement features (versioning, permissions, linking) at the Block level and have them apply to any content type. It also simplifies the collaboration engine – whether users edit a paragraph or an entire page, it's just editing one or multiple Blocks. Furthermore, it facilitates the knowledge graph: blocks are nodes in a graph, which we can connect with typed edges to represent semantic relationships (ADR 4). This is aligned with modern knowledge graph principles where **nodes represent entities and edges represent typed relationships**[[1]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b). Making every content piece a node (Block) allows rich semantic linking (e.g. a "Claim" block supported by an "Evidence" block via a "supports" edge).

The unified block approach draws inspiration from block-based editors and the idea of _"atomic" knowledge units_. It avoids hardcoding distinctions like "page vs paragraph" – instead, a page is a block that happens to have children. This simplifies the backend: one Convex table and API for all content. It also eases future extension: new content types (code cells, mathematical blocks, etc.) can be added as new block types without altering the fundamental data model.

## Alternatives Considered

- **Page/Document Model:** Treat entire documents or articles as single records (with a large text field). This was simpler for basic editing but lacks granularity – individual ideas or sections could not be linked or versioned independently. It would also complicate collaborative editing on sub-sections.
- **Separate Tables per Content Type:** e.g. a documents table, paragraphs table, images table. This adds complexity and duplication of logic for each type. We preferred a single table with a type field for simplicity and consistency.
- **Embedded Hierarchy in One Document Blob:** Store a whole document's content (with internal blocks) as one JSON structure (as ProseMirror does internally). This would simplify retrieval but make reusing or referencing parts of the content harder. We want Blocks addressable individually (e.g. link to a specific block in another document).
- **Graph Database or RDF Triple Store:** Using a dedicated graph DB or RDF model for all knowledge. This could handle nodes/edges elegantly but introduces additional infrastructure and complexity for the MVP. Instead, we implement a graph model on Convex (storing edges in a table, see ADR 4) which is conceptually similar to the typical SQL approach of nodes and an edges table[[2]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,offs) without needing a separate DB.

## Implications

- **Atomic Reuse:** Because everything is a Block, we can support reusing or transcluding a block in multiple contexts. For example, the same "Experiment Result" block could be referenced in two different documents via edges instead of duplicated content. (Full transclusion UI is post-MVP, but the model permits it.)
- **Performance Considerations:** Loading a full document now requires fetching multiple blocks (the container plus its children). Convex doesn't support JOINs, but we can retrieve children by indexing edges or storing child IDs. We'll likely fetch blocks in batches by their IDs. As documents are edited, many small block updates occur instead of one large document update, but Convex's real-time engine can handle frequent small mutations.
- **Consistency and Transactions:** Changes that span multiple blocks (e.g. splitting a paragraph into two blocks) need to be handled carefully in Convex functions to keep data consistent. Convex allows sequential operations in a transaction (single function call), which we'll use to e.g. insert a new block and update an edge or parent reference atomically.
- **Simplified Feature Implementation:** Features like per-block versioning (ADR 2) and per-block permissions (ADR 6) are straightforward because the "block" is the unit of change and access control. However, we must consider that a user viewing a page needs permission for all its constituent blocks – our queries or resolvers must filter out or aggregate permissions accordingly.
- **Uniform API:** We can build generic APIs (e.g. getBlock(id), updateBlock(id, content)) that work for any type of content. Fewer code paths mean fewer bugs. On the flip side, some types (images, files) have very different handling (via file storage in ADR 5), but they still share metadata and ID structure with text blocks.

Expansion Path

- **New Block Types:** As we extend the system, we can introduce new block types (for example, a "code block" with executable code, or a "chart" block). The ADR decision scales: just add a new type constant and define how its content is structured. The rest of the system (edges, versioning, permissions) already works with it.
- **Federation of Blocks:** In the future, if we allow a federated architecture (multiple servers or instances sharing knowledge), the block model is well-suited to become the unit of federation. Each block can carry a global identifier (possibly a composite of instance + ID) to be referenced across instances. We would need a mechanism to sync or fetch remote blocks on demand in a federated scenario, and a way to resolve conflicts if the same block is edited in different instances (which might entail CRDTs, see ADR 3 Expansion).
- **Block Caching and Local-First:** Because blocks are small, a client could cache frequently used blocks (or even the entire knowledge base if not huge) for offline use, then sync changes to Convex. This leans into a local-first future. Convex's **immutable persistent data structures** (per their whitepaper) could support such workflows, though we are not implementing offline in MVP.
- **Semantic Enrichment:** With every content piece as a block, we can attach semantic data easily. For example, one could tag a block as "hypothesis" or "evidence" via a property, enabling an AI agent to treat blocks differently. This fits the block model and can later be leveraged by agent frameworks or AI (e.g. an agent could retrieve all blocks tagged "research question").
- **Optimized Queries:** As the number of blocks grows, we might introduce indexing or partitioning. For instance, we could partition blocks by project or space to keep query loads manageable. Convex allows creating secondary indexes on table fields; we will likely add an index on parentId or similar to efficiently query a document's blocks. In the future, if some operations require graph traversals, we might consider integrating a graph query library or migrating frequently traversed relationships to a graph-optimized store – but currently Convex + our edges table should suffice.

ADR 2: Versioning Strategy (Steps, Snapshots, and User-Facing Versions)

Context

In a collaborative editing environment, tracking versions of content is critical. Users will be concurrently editing blocks, and we need to record those changes for real-time sync, history, and potential rollbacks. The challenge is to manage _fine-grained changes_ (so that realtime merging is smooth) while also providing _meaningful version history_ to users without overwhelming them with every tiny edit. Additionally, loading a document should be efficient – we cannot reapply an infinite log of operations from the beginning for each new collaborator. We need a strategy that balances **granularity, performance, and clarity** in versioning.

Decision

**Adopt a three-tier versioning approach:** (1) **Steps** as atomic changes for real-time collaboration, (2) **Snapshots** as periodic full-state checkpoints for efficiency, and (3) **User-Facing Versions** as labeled milestones in history.

- **Steps (Operational Changes):** We record every edit operation as a _Step_. In the context of ProseMirror, a Step is a small change like "insert text at position X" or "delete 5 characters at range Y". Each step is associated with the specific Block or document being edited and carries the base version it was applied to. Steps are stored in a Convex table (e.g. steps) with fields: {documentId, stepData, version, clientId, timestamp}. We assign a monotonic **version number** to each document which increments with each confirmed step, similar to ProseMirror's linear version counter[[3]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=changes%2520in%2520a%2520different%2520order,will%2520produce%2520the%2520same%2520document). This version is essentially the "source-of-truth" state index.
- **Snapshots (Periodic Checkpoints):** We regularly capture the entire content state as a Snapshot. A snapshot might be the full ProseMirror document JSON for a given block/document at a certain version. Snapshots are stored in a snapshots table: {documentId, version, contentState, timestamp}. We implement a **debounced snapshot** mechanism: after a flurry of edits, when a document has been idle for a short interval (say 1 second) and the last editor was the current user, the client will package up the full document state and send it to the server as a snapshot[[4]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,side,%2520enabling%2520easy%2520AI%2520interoperation)[[5]](https://github.com/get-convex/prosemirror-sync%23:~:text=const%2520prosemirrorSync%2520=%2520new%2520ProsemirrorSync\(components,//). This means we snapshot only stable states to avoid excessive overhead. New collaborators can then load the latest snapshot and skip applying a long chain of old steps[[6]](https://github.com/get-convex/prosemirror-sync%23:~:text=Configuring%2520the%2520snapshot%2520debounce%2520interval).
- **User-Facing Versions:** To avoid exposing every low-level step to end users, we define higher-level version markers. These could be automatically generated (e.g. daily versions or after a significant pause in editing) or manually created ("Save Version" action). A user-facing version basically points to a particular snapshot or a checkpoint in the step log with a friendly name or number. We might maintain a list of these in a versions log with metadata (author, label, timestamp, snapshot reference). In MVP, we can treat snapshots themselves as the user-visible versions (since they represent document states at certain times). Eventually, we can allow users to tag certain snapshots (or create one on demand) to serve as notable versions.

Under this scheme, **the document's state is reconstructed by starting from the latest snapshot and applying subsequent steps**. A brand new client will fetch (via Convex) the latest snapshot and the list of steps after that snapshot. This prevents needing the entire history since inception. We will also implement **history truncation** — older steps that are no longer needed (because a snapshot exists after them) can be archived or deleted to save space[[4]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,side,%2520enabling%2520easy%2520AI%2520interoperation). The Convex ProseMirror sync component even includes an API for deleting old snapshots/steps safely[[7]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,for%2520old%2520snapshots%2520&%2520steps), which we will leverage or mimic.

Rationale

This multi-tier approach is inspired by best practices in collaborative editing and source control:

- **Fine-grained OT for concurrency:** The Step log (with version numbers) is essential for real-time OT (Operational Transform) style merging. By assigning each change a version and rebasing out-of-order changes, we ensure a linear history as ProseMirror's algorithm expects[[8]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Like%2520OT,%2520ProseMirror%2520uses%2520a,will%2520produce%2520the%2520same%2520document). If we only saved infrequent whole-document versions, we'd lose detail needed to merge concurrent edits correctly.
- **Snapshots for performance:** Over time, a document might accumulate thousands of steps. Replaying all from the beginning would be slow. Snapshots let a new user start from a recent state and avoid replaying an entire history[[4]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,side,%2520enabling%2520easy%2520AI%2520interoperation). This is analogous to database checkpointing or the way Git uses full tree objects on commit – it bounds the work needed to reconstruct state.
- **User-friendly version history:** A raw list of every keystroke (step) is not useful to humans. Snapshots provide more meaningful states (e.g. after a paragraph was finished). By exposing selected snapshots (or combining steps into larger "commits"), we give users a comprehensible version history. This aligns with the idea of **commit-based collab** where multiple step updates are grouped into one commit on the backend[[9]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=The%2520gist%2520is%2520that%2520we,them%2520back%2520to%2520the%2520clients). We plan to **batch consecutive steps** from one user into logical commits when possible, so the history isn't flooded with trivial edits. For example, typing a word might produce several character insert steps, but we can treat the whole word insertion as one version when viewing history.
- **Compatibility with ProseMirror and Convex:** ProseMirror's collaboration schema already includes the concept of steps and versions, and the Convex _prosemirror-sync_ component implements snapshots and step handling with a debounce[[10]](https://github.com/get-convex/prosemirror-sync%23:~:text=,side,%2520enabling%2520easy%2520AI%2520interoperation)[[11]](https://github.com/get-convex/prosemirror-sync%23:~:text=Configuring%2520the%2520snapshot%2520debounce%2520interval). By following this model, we leverage known, tested patterns. The "version" integer is simple to work with (monotonic counter) and fits nicely in Convex data (just a number to compare).
- **Recovery and Auditing:** With this strategy, we can retrieve any past version of a document by applying steps to a snapshot, facilitating features like "view history" or "undo to an earlier version". It also provides an audit log of changes (who made what edit when, if we include user IDs in step records). Storing snapshots periodically also guards against cases where a long sequence of operations might otherwise be lost or too costly to recover.

Alternatives Considered

- **No Fine-Grained History (Last-write wins):** Simply store the latest content state for each block, overwriting on edits. This was rejected because it cannot support collaborative editing or undo. It would also lose all history – unacceptable for our knowledge system where tracking changes is important.
- **Only Storing Full Snapshots (Document per Version):** We considered saving a full copy of the document for each significant change (like a version history without steps). While simple, this would be very storage-inefficient for large content and makes merging concurrent edits hard (you'd have to do content diff/merge, effectively re-implementing OT or CRDT from scratch).
- **CRDT-based Versioning (e.g. Yjs):** CRDTs natively allow merging and have an inherent version-less conflict-free history. We decided against this for MVP because it complicates integration with ProseMirror's schema and increases data size (CRDT tombstones, etc.). ProseMirror's own collab approach (OT with central authority) is sufficient and simpler given a centralized server[[12]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=The%2520design%2520decisions%2520that%2520make,be%2520interesting%2520to%2520work%2520on). We do note CRDT as a future option if offline editing or decentralized operation is needed (see Expansion Path).
- **Git-like Tree of Commits:** Another idea was to treat each saved version as a commit in a chain, possibly branching for concurrent edits and then merging. This is powerful (allows branching/forking documents) but adds a lot of complexity to the model and UI. We opted for a linear history for MVP, aligning with ProseMirror's linear version (which "can be denoted by an integer" and does not natively support divergent branches in real-time collab)[[13]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=changes%2520on%2520top%2520of%2520them,,before%2520retrying%2520the%2520push).

Implications

- **Storage Growth:** Every edit generates a step record, and periodic snapshots duplicate the document content. We must manage storage by pruning old data. We will likely implement a **retention policy**, e.g., keep all steps for the last X days and snapshots for key versions, but purge older fine-detail if space becomes an issue. The Convex collaboration component provides an API to delete old snapshots/steps which we can use[[7]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,for%2520old%2520snapshots%2520&%2520steps).
- **Conflict Resolution UX:** At the technical level, conflicts are handled by OT (rebasing). However, from a user perspective, simultaneous edits can produce confusing results in the content (e.g., interleaved text). Our versioning captures what happened but doesn't _prevent_ overlaps. Users might see content change rapidly if two people edit the same sentence. The version history will show both edits sequentially. We might need to highlight conflicts or at least ensure presence indicators (ADR 7) warn users when editing nearby.
- **Version Numbers and Client State:** Each block or document will have a current versionNumber. Clients must track their local version and on each sync with server, update it. If a client is offline and comes back, it will need to fetch all steps since its last known version. This is supported by our model (get steps > version X from Convex). However, if a client falls too far behind (e.g., their last version snapshot was vacuumed away), we'll send a snapshot instead (the server can detect if the number of missing steps is large and send a snapshot to re-sync).
- **User Interface for History:** Initially, we might not expose a full history UI to end users, but internally we have the data. Down the line, showing a timeline of changes or allowing "undo to here" will be possible by leveraging snapshots and steps. We will have to reconcile how to display batched steps (we may group steps by time or by author when showing history).
- **Audit and Attribution:** Each step can carry the user ID of its author and a timestamp. This allows us to build an audit log for each block. It's important for knowledge systems to know who contributed what. Storing this at step granularity means a very detailed audit. We might summarize it at snapshot/version level for ease (e.g., "Alice edited this paragraph at 10:45, Bob edited at 10:47").

Expansion Path

- **CRDT or Offline Support:** In the future, if we need offline editing or a decentralized server network, we might shift from pure OT to a CRDT-based approach (e.g., Yjs). CRDTs don't require a central source-of-truth and can merge changes without explicit snapshots. We would then store CRDT states or updates instead of ProseMirror steps. The current architecture can accommodate this by swapping out the collaboration engine (see ADR 3 expansion) – possibly using a Yjs-specific Convex component if available[[14]](https://github.com/get-convex/prosemirror-sync%23:~:text=,and%2520doesn't%2520have%2520local%2520changes). This would let multiple servers or offline clients converge state without a linear version.
- **Named Versioning and Branching:** We plan to introduce the concept of "named versions" (like git tags or branches) for major milestones. For example, a user could mark a block or document as "v1.0 Published". This could simply create a snapshot with a label. In the future, branching (forking a block into two divergent versions) could be supported for scenarios like proposing alternate edits or forking knowledge in different directions. Our step/snapshot model would then need to maintain multiple parallel logs, which is a non-trivial extension but conceptually possible by keying steps to branch IDs.
- **Global Version Clock or Federated Versioning:** If blocks become referenced across different domains or systems, we might need a more global version identification. For instance, if two different Convex deployments exchange a block, how to merge their edits? A potential expansion is using something like **Lamport timestamps or vector clocks** for versioning in a federated setting instead of a single integer. This would pair with CRDT approaches if federation is needed.
- **Optimized Batch Commits:** Building on the idea of step batching, we may implement a server-side commit aggregation. For example, instead of immediately broadcasting each individual step, the server could accumulate steps from multiple users that are based on the same prior version and apply them in one logical commit (transforming them as needed). This approach, as noted by research[[9]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=The%2520gist%2520is%2520that%2520we,them%2520back%2520to%2520the%2520clients), can reduce the problem of high-latency users being starved out by continuous conflicts. Implementing this would mean the server sometimes accepts out-of-order steps and internally transforms them to avoid rejecting them (which is a more advanced algorithm). If needed, we can iterate towards this to improve collab smoothness under heavy load.
- **Integration with Workflow/AI (Mastra):** Version history could be leveraged by AI agents (see ADR 8). For example, an agent could analyze the diffs between versions to summarize changes or to revert problematic edits automatically. Our structured history would enable such capabilities. We might add **difference storage** (precomputed diffs) for quick comparisons if that becomes a frequent operation for agents or UI.

ADR 3: Collaboration Protocol (ProseMirror OT, Step Batching, Conflict Handling)

Context

Multiple users need to **edit the same block or document in real-time**, seeing each other's changes instantly. We require a collaboration protocol that merges concurrent edits reliably. ProseMirror was chosen as the rich-text editing framework, and it comes with a collaboration module based on Operational Transformation (OT) in a centralized fashion. We need to integrate this with our Convex backend. Key challenges include: ordering of changes (to avoid divergent states), network latency differences, and conflict resolution (ensuring changes that happen concurrently are applied in a consistent way for all users). We also want to mitigate the known issue of **high-latency users** suffering from repeated OT rebases (where their changes get rejected and must retry if someone else's edit got in first)[[15]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=During%2520sessions%2520with%2520a%2520high,and%2520the%2520client%2520backlogs%2520clear). The protocol design must balance real-time responsiveness with fairness and consistency.

Decision

**Use ProseMirror's centralized OT-based algorithm for collaborative text editing, enhanced with batched step commits on the server to reduce conflict thrash.** In practice, this means:

- We run a **single source of truth** for each document's state on the Convex backend (the "authority" in OT terms). Clients do not directly sync with each other peer-to-peer; all changes funnel through Convex.
- Clients use the ProseMirror collaboration plugin or our Convex-prosemirror integration. Each client maintains its own local version number of the document. When a user makes an edit, it generates one or more ProseMirror Step objects. The client **optimistically applies** the step locally and sends it to the Convex function (e.g. submitSteps) along with the version number the client based it on.
- The Convex server function receives these steps and checks the version.
- If the steps' base version matches the server's current version for that document, the server accepts them. It increments the document's version and records the steps in the steps table (ADR 2) as committed. Then it broadcasts the steps to all other subscribed clients (Convex's real-time push will notify them).
- If the base version is outdated (another user's steps arrived first), the server will **reject** the incoming steps for now. The client is notified (or will infer, when it pulls the latest changes and sees its own changes not applied) that it must rebase. The client will fetch the new steps it missed (Convex provides getSteps from last seen version) and use ProseMirror to **rebase its unconfirmed steps** on top of the latest document state[[16]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=By%2520using%2520a%2520central%2520server,,them,%2520before%2520retrying%2520the%2520push). After rebasing locally (which uses position mapping to adjust the step to the new content[[17]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Position%2520Mapping)), the client resends the step. This loop continues until the step is accepted.
- We plan to implement **step batching** on two levels:
- **Client-side batching:** The client will bundle multiple local steps into one submission if they occur in quick succession. For example, ProseMirror's collab plugin already collects "unconfirmed" steps and can send them as one batch to the server periodically[[18]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=ProseMirror,at%2520which%2520point%2520they%2520must). We will ensure this is tuned (e.g., send every 50-100ms or on text input pause) so that we reduce overhead and avoid flooding the server with single-character steps.
- **Server-side commit batching:** In the basic OT model, only one client's changes are accepted for a given document version and others get rejected to rebase, which can starve slow connections[[15]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=During%2520sessions%2520with%2520a%2520high,and%2520the%2520client%2520backlogs%2520clear). To improve this, our Convex function will attempt to **merge concurrent submissions** instead of outright rejecting. Specifically, if two sets of steps arrive nearly together on the server (both based on version N), we can choose one as the winner for version N+1, but immediately take the second and transform it against the first (using ProseMirror's step transformation logic) rather than rejecting it. We then apply that transformed second set as version N+2 without a round-trip to the client. In effect, the server can do the rebase for the client and accept both in sequence. This approach aligns with the _commit-based collab_ idea, where the server applies commits in order they arrive and maps other concurrent ones accordingly[[9]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=The%2520gist%2520is%2520that%2520we,them%2520back%2520to%2520the%2520clients). It avoids the extra latency of clients having to resend. We will start with simple cases (two edits both inserts in different locations can be accepted back-to-back). In complex conflicts (e.g., both edit exactly the same character), the server may still resort to letting one win and one rebase client-side, to keep logic simpler initially.
- **Conflict handling policy:** By default, if two users edit the same text region, the first edit that reaches the server will be applied; the second edit will be transformed. ProseMirror's OT will ensure no document corruption, but the resulting text might intermix changes. Our policy is last-write-wins for overlapping text insertion/deletion conflicts (since the second writer's change is rebased on a document that includes the first writer's change, it effectively comes after). We do not implement any semantic conflict resolution beyond what OT provides – e.g., we won't try to merge two different words typed at the same spot; the OT result (usually both words with one following the other) stands, and it's up to users to manually clean up if needed. The key is all users see the same result.
- We use **ProseMirror's position mapping** to handle concurrent insert/delete adjustments. For instance, if User A inserts a character at position 5 and concurrently User B deletes characters 10-12, when B's delete is applied after A, the delete positions are remapped (+1 offset) to account for A's insert[[19]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=1,front%2520of%2520the%2520change's%2520offset)[[17]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Position%2520Mapping). This ensures consistency.
- **Awareness and locking:** We do not implement hard locks on document regions (no pessimistic locking). However, via presence (ADR 7) we will show if someone else is editing a paragraph, to encourage users to avoid editing exactly the same line. This is purely a UX courtesy; the system itself allows concurrent edits anywhere.
- Our Convex backend will use the **@convex-dev/prosemirror-sync** **component** as a foundation. This component provides server functions for submitting steps and snapshots and handles much of the logic above (including version checks and rebase instructions)[[10]](https://github.com/get-convex/prosemirror-sync%23:~:text=,side,%2520enabling%2520easy%2520AI%2520interoperation)[[5]](https://github.com/get-convex/prosemirror-sync%23:~:text=const%2520prosemirrorSync%2520=%2520new%2520ProsemirrorSync\(components,//). By using it, we get a proven implementation of ProseMirror's OT algorithm in Convex. We will extend or configure it to add the server-side commit batching described.

Rationale

We chose ProseMirror's OT-based collaboration because it is a well-established solution that fits our centralized backend model. Key reasons:

- **Centralized OT is simpler and consistent:** Unlike fully distributed OT, a centralized approach is "relatively easy to implement and reason about"[[20]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=But%2520you%2520can%2520save%2520oh,their%2520Google%2520Docs%25E2%2580%2594a%2520centralized%2520system). We have a single server ordering events, avoiding the complexities of multi-leader consensus. As Marijn Haverbeke (ProseMirror's author) notes, a central authority allows using a linear history like in version control, with clients rebasing their own changes when needed[[8]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Like%2520OT,%2520ProseMirror%2520uses%2520a,will%2520produce%2520the%2520same%2520document). This yields one definitive sequence of edits, which is exactly what our versioning (ADR 2) stores.
- **Leverages ProseMirror's algorithms:** We avoid reinventing the wheel. ProseMirror provides the transformation logic and the collaborative plugin that manages unconfirmed vs confirmed steps. By aligning with that, we ensure our system benefits from years of testing of those transformations. For example, ProseMirror has robust mappings to keep cursors in place through changes[[17]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Position%2520Mapping), which we will utilize in conflict resolution and presence (so a user's cursor moves appropriately when other text is inserted above).
- **Real-time performance:** OT with immediate local application of changes means users see their own typing without delay, and others' edits as soon as network allows. The overhead is minimal JSON diff (steps) and our data model is optimized for that (small step objects). Convex's WebSocket push will deliver updates promptly to all subscribers.
- **Step batching for fairness:** We address the problem observed in high-contention editing sessions where a user with higher latency can become a "starving artist" with their changes constantly rejected[[15]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=During%2520sessions%2520with%2520a%2520high,and%2520the%2520client%2520backlogs%2520clear). By batching, we _reduce the frequency of version mismatches_. If 5 users type at once, rather than 5 separate rapid version increments causing collisions, we effectively coalesce changes either on the client or server. The commit-based enhancement (transforming concurrent commits instead of rejecting) is drawn from research that showed it's possible to avoid many client retries and thereby improve throughput[[21]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=There%2520are%2520a%2520number%2520of,a%2520lot%2520of%2520extra%2520complexity)[[22]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=Can%2520we%2520eliminate%2520the%2520need,work%2520closer%2520to%2520the%2520database). This rationale is to improve _user experience under load_: fewer "your changes were overwritten, retrying…" glitches.
- **Alternatives seen as riskier for MVP:** We considered CRDT (like Yjs) but OT was deemed sufficient and simpler given the client-server architecture. Also, ProseMirror's built-in OT means we didn't need to integrate a new library or change data structures drastically.
- **Convex integration:** The existence of the Convex ProseMirror sync component[[10]](https://github.com/get-convex/prosemirror-sync%23:~:text=,side,%2520enabling%2520easy%2520AI%2520interoperation) significantly de-risks this approach. It gives us an out-of-the-box implementation of server-side OT (with functions for submitting steps, fetching steps, handling snapshots) that we can use and then tweak for our needs. This alignment speeds development and ensures correctness, as opposed to writing our own collaboration engine from scratch.

Alternatives Considered

- **CRDT (Conflict-free Replicated Data Types):** Using a CRDT-based editor (like TipTap with Y.js or similar) was a strong alternative. CRDTs allow true peer-to-peer collaboration and offline edits merging without a central server ordering. However, integrating CRDT in ProseMirror (there is a Y.js plugin for ProseMirror) would introduce heavy data overhead (each character insertion carries an identifier, etc.) and complexity in our storage. Also, real-time OT is already solved by ProseMirror for our use case. We decided to stick with OT now and possibly switch to CRDT if offline capabilities are needed later. Notably, the Convex team suggests a Yjs-specific component could be separate[[14]](https://github.com/get-convex/prosemirror-sync%23:~:text=,and%2520doesn't%2520have%2520local%2520changes), implying that mixing OT and CRDT would be non-trivial.
- **Pessimistic Locking:** We could lock a block or section while a user is editing it, to avoid conflicts entirely. This was rejected because it severely hampers collaboration – users would have to wait turns or break documents into tiny locked sections. It doesn't match the freeform concurrent editing experience we want.
- **Third-party collab services:** There are services like Firebase OT, ShareDB, or even using something like Automerge with a central relay. Adopting those would either require replacing ProseMirror's editing behavior or bridging to it. Since ProseMirror already solved this within its ecosystem, adding another layer felt unnecessary.
- **Do Nothing Special (last write wins):** In theory we could let Convex handle concurrent writes by last-write-wins (the last update to a field wins) without OT. This would be disastrous for text merging (lost updates, jumbled text) and is not a real solution for simultaneous editing, so it was not seriously considered.

Implications

- **Server Load:** Every keystroke potentially results in a Convex function call and database writes (steps). For large collaborative sessions, this can be heavy. We mitigate this with client-side batching (fewer, larger step submissions) and by keeping steps small. Convex's scalability will be tested; however, Convex is designed for real-time updates and can handle many small writes with its **WebSocket reactivity and caching**[[23]](https://stack.convex.dev/presence-with-convex%23:~:text=Convex%2520helped%2520make%2520this%2520much,WebSocket%2520reactivity%2520and%2520caching%2520scalability)[[24]](https://stack.convex.dev/presence-with-convex%23:~:text=Caching). We will monitor performance, and we might need to tune batch sizes or use Convex's single-flight mechanism to drop intermediate state updates if flooding occurs (similar to presence, see ADR 7).
- **Complexity in Edge Cases:** While ProseMirror OT covers most editing scenarios, certain edge cases like _simultaneous step submission and snapshot submission_ could occur. The convex component has logic to handle snapshot submission concurrency (only last editor triggers snapshot)[[11]](https://github.com/get-convex/prosemirror-sync%23:~:text=Configuring%2520the%2520snapshot%2520debounce%2520interval)[[25]](https://github.com/get-convex/prosemirror-sync%23:~:text=There%2520can%2520be%2520races,%2520but,and%2520are%2520safe%2520to%2520apply). We need to ensure our usage is correct to avoid race conditions (e.g., two users both think they're last to edit and send snapshots – one will win, the other should be discarded safely).
- **Data Consistency:** We must be careful that the Convex functions that modify documents (applying steps) are **atomic** per document. Convex functions run isolated, so if two calls happen at once on the same doc, one will naturally serialize after the other (Convex ensures consistency in its transaction model). But if we introduce custom logic for merging steps on the server, we have to implement it in that single function call to maintain atomicity. It complicates the server function a bit (it might have to handle an array of step sets).
- **Client Complexity:** Clients need to maintain two sets of state: the optimistic local state and the confirmed server state. This is handled by the ProseMirror collab plugin, but developers must be careful to initialize editors with the correct starting document (from latest snapshot + steps). There is also complexity in error handling – if a client falls far behind (maybe missed many updates), it should reload the document state from scratch. We will likely implement a safety where if a client's version is too old (server doesn't have those old steps anymore), we send them a fresh snapshot to resync.
- **Potential Merge Artifacts:** The OT algorithm doesn't attempt to semantically merge content. So, documents might end up with odd artifacts, like both users typed the same word -> after merge the word appears twice. Or both deleted a line -> after merge, one delete might target text that's already gone, resulting in a no-op for the second (so one deletion "wins" but effectively both wanted to delete, which is fine). These artifacts are generally minor and can be edited out by the users. We will document this known behavior in internal notes so it's understood that the system ensures consistency but not intelligent merging. This is similar to how Google Docs works – simultaneous edits yield a combined result, which users may adjust if needed.
- **Integration with Non-Text Blocks:** Collaboration protocol so far is described for rich-text content (ProseMirror docs). For other block types (images, attachments), "editing" might mean something else (e.g., changing an image's caption or moving it). Those operations might not use ProseMirror at all. We will handle them with simpler strategies: e.g., if two users try to edit an image block's metadata concurrently, the last write might simply win, since such edits are rare and typically not simultaneous. The ADR 3 protocol primarily covers the heavy case of text editing. We will note that any non-text collaborative actions (like two users reordering blocks at the same time) need thought – possibly we treat those also as operations (e.g., an operation to move block X after block Y, which we could sequence similarly).

Expansion Path

- **True Offline Collaboration:** To support offline work, we might integrate a CRDT-based approach like Yjs. In that scenario, each client could make changes without the server and later merge. We'd need to reconcile that with our current OT log. Possibly, we could run a Yjs document in parallel that syncs to Convex. Alternatively, switch fully to Yjs: the Convex backend could store Yjs updates (deltas) and broadcast them. This would remove the need for central OT and allow federated editing, but we'd have to migrate data structures (ProseMirror content to Yjs format). This is a non-trivial change, but if the product requires offline use or multi-master (federation) editing, it might be necessary.
- **Federated Servers and Consensus:** If in future the knowledge base is distributed across servers (no single authoritative server), we could explore using a consensus algorithm (like Raft or CRDTs) to maintain a consistent ordering of operations across sites[[26]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=And%2520I%2520don't%2520actually%2520believe,have%2520not%2520actually%2520tried%2520this). For example, a primary Convex node could be elected among a cluster to order the steps, or use operational transform in a federated setting by having one "arbiter" site. This is complex and likely beyond near-term needs, but the ADR notes it for completeness. Our current design keeps one authority per document (the Convex deployment owning it).
- **Intelligent Merge Assistance:** We could introduce an AI assistant to help with conflict resolution. For instance, if two users edit the same sentence differently, an AI could detect this and suggest a merged version or alert the users. This would layer on top of our existing protocol (the data is there to see the two divergent edits). This could be part of an "AI co-editor" feature implemented via Mastra agents (ADR 8).
- **Scale to High User Counts:** Currently, ProseMirror is optimized for maybe dozens of concurrent users editing a single document. If we ever have a scenario of hundreds collaborating (like a massive shared notepad), we may need to adjust. Step batching becomes crucial, and perhaps limiting update frequency. We might implement a token-bucket or turn-taking at the application level if things get too chaotic. Another optimization could be **region locking on demand** – not mandatory, but if a document is very busy, the system might automatically partition it (for example, lock down a paragraph if too many people try to edit it simultaneously, asking them to queue edits). These are speculative and only if we hit scale issues.
- **Multi-cursor / Multi-region Editing:** ProseMirror by default operates on one document. In a complex scenario, say an outliner with many blocks, perhaps multiple blocks could be edited independently at the same time (since each block is separate content). We plan to treat each top-level document block as separate collab sessions. In expansion, we could have multiple simultaneous OT sessions (one per block being actively edited). Our protocol can handle that by isolating steps per document/block. We just need to ensure consistency if an operation spans blocks (which currently doesn't happen in MVP – each operation is within one block's content).
- **Integration of Comments or Suggestions:** Collaboration isn't just direct edits. We may later add a feature for suggestions (track changes) or inline comments. These add another dimension to collab. We'd extend our model by treating comments as data attached to ranges in the text. We'd then need to ensure that as text changes, comments move accordingly – ProseMirror's mapping can help with that, similar to how it keeps cursor positions stable[[17]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Position%2520Mapping). This likely means storing comments in a structure that references document positions that get updated via step maps on each edit. It's an advanced expansion, but our underlying collab engine would support it since we can map positions through changes reliably.

ADR 4: Edge Types and Dialectical Graph (Typed Edges Over Blocks)

Context

Beyond linear documents, our knowledge system aims to represent a **knowledge graph** – a network of ideas and information. Blocks (from ADR 1) are the nodes of this graph. We need a way to connect these nodes with various relationships. Some connections are purely structural (e.g. a document block _contains_ a paragraph block), while others are semantic (e.g. Block A _supports_ Block B, or Block C is a _reference_ cited by Block D). The term "Dialectical Graph" suggests we want to model argumentative or dialogic relationships: for example, hypotheses and evidence, claims and counterclaims. In essence, we need **typed edges** between blocks to capture rich relationships and to enable traversal of knowledge in a graph-like manner rather than just parent-child hierarchies.

Decision

**Implement a general Edge model in the Convex database, with a type field to characterize the relationship, thereby forming a directed graph of blocks.** Each edge will be a record in an edges table (or multiple tables by category if needed for indexing) with schema roughly:

- id: Id<"edges"> – primary key for the edge.
- from: Id<"blocks"> – the source block (start node).
- to: Id<"blocks"> – the target block (end node).
- type: string – the relationship type (examples: "contains", "references", "supports", "opposes", "answers", etc.).
- Metadata: e.g., createdBy, createdAt, perhaps an optional annotation or weight.

Edges are directed: an edge from A to B means "A [type] B". By using types, we can express **dialectical relationships** explicitly. For example: - If Block X is a claim and Block Y is evidence supporting it, we create an edge Y --supports--> X. - If Block M contradicts Block N, an edge M --refutes--> N (or opposes) is used. - For structural containment, we use contains: e.g. Document D contains Block P (edge D --contains--> P). This could also cover sequences or parent/child relationships. (We might manage order via an index or separate field, since multiple edges of type "contains" from the same parent should have an order – possibly a position property in the edge or content array in the parent block.) - We can represent a Q&A link: Question block Q is answered by Answer block A via A --answers--> Q. - Dialectical or debate mapping: e.g., thesis --counters--> antithesis if needed.

This design treats edges as first-class objects with their own types and potential properties. It aligns with a **property graph model**, where relationships have a type and can have attributes[[1]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,interrelate). We will define a core set of allowed edge types for MVP: - **Structural types:** "contains" (or "parent-of"/"child-of" relationship). Possibly also "next" for ordering if linear order isn't encoded by a list. - **Reference type:** "references" (Block A references Block B, e.g. a citation in text linking to a source block). - **Support/Oppose types:** "supports", "refutes" to capture dialectical support or contradiction. - **Association types:** "related" for a loose link, "example-of", "definition-of" etc., depending on our domain needs. We will likely enumerate these in code (as constants or an enum) to avoid arbitrary strings. However, the model is flexible to add new types later without schema changes.

Edges will be manipulated via Convex mutations: e.g., addEdge(from, to, type) to create a link, and removeEdge(id) to delete. We'll also have queries like getEdgesFrom(blockId, typeFilter?) or getEdgesTo(blockId, typeFilter?) to traverse the graph. We plan to create Convex indexes on the from and to fields for efficient lookup (since a typical query is "find all edges where from = X" or "to = Y").

Rationale

A typed edge model is chosen to provide **semantic richness** and queryability to the relationships in the knowledge base:

- **Separation of content and relationships:** By storing edges separately from blocks, we keep blocks focused on their content and use edges to handle links. This makes it easy to add or remove relationships without modifying the block data. It also means one block can participate in many relationships (one-to-many) freely.
- **Typed edges allow context-specific logic:** For instance, when rendering a document, we follow "contains" edges to gather its content blocks in order. When visualizing an argument graph, we specifically look for "supports" or "refutes" edges to display pro/con arguments. If all links were untyped, we'd lose this ability to treat different connections appropriately[[27]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,specific%2520information). Knowledge graphs benefit from **rich semantic relationships** rather than just generic links.
- **Evolvability:** We anticipate expanding the relationship types as the system grows (especially when integrating AI or external systems). The chosen design is easily extendable by adding new type values, akin to adding new relationship types in a property graph – it doesn't break existing data.
- **Query and analysis:** With edges in a table, we can perform graph-like queries within Convex. For example, to find all evidence supporting a claim, we query edges where to = claimId AND type = "supports". This could be done in a Convex query function, potentially even reactive if needed. If edges were only implied (say by embedding references inside block content), we'd have a much harder time querying or enforcing consistency.
- **Alignment with knowledge graph practices:** Storing nodes and edges in tables is a common approach if not using a specialized graph database[[2]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,offs). We keep that simplicity (just two tables: blocks and edges). It also resonates with how one might export the data to RDF or other formats in future – each edge is essentially a triple (subject, predicate, object).
- **Dialectical reasoning:** Specifically for dialectical or argument maps, typed edges allow implementing logic such as "if evidence block E supports claim C, and claim C supports theory T, then E indirectly supports T" – these inferences or UI presentations rely on understanding the edge types. We might later have features that automatically summarize support/contradiction for a given block. Having edges clearly labeled with "supports"/"refutes" will enable algorithms or agents to traverse and aggregate arguments.

Alternatives Considered

- **Untyped (or Implicit) Links:** We could have had a simpler model where blocks just store references to other blocks (like an array of linked IDs) without specifying type, or where type is implicit by context (e.g., a "reference" could just be a hyperlink in text pointing to another block's ID). This was not chosen because it lacks clarity – we want the system to _know_ what a link means. Typed edges make knowledge machine-interpretable (so, for example, an agent can find all counter-arguments easily).
- **Edge Properties embedded in Blocks:** Another approach is to have each block contain fields for relationships, for example: a block might have citations: [blockId] for references or supports: [blockId] for the claims it supports. This becomes unwieldy because each new relation type adds a field, and many-to-many relations are harder to normalize (should the claim list all supporting blocks or each support list which claim it supports? That duplication invites inconsistency). A separate edge entity avoids these problems by normalization.
- **Graph Database or External Store:** We considered using a dedicated graph engine or external service for relationships. For MVP, it's overkill. Convex is fully capable of handling these as rows, and given our data volume in MVP, performance is fine. Using Neo4j or similar could be considered if queries become very complex (like multi-hop traversals) and if Convex queries prove too slow for that. But we prefer to avoid additional infrastructure.
- **Fixed Ontology vs Flexible Types:** We debated whether to allow _user-defined_ edge types (folksonomy-style) or stick to a fixed set. MVP will use a fixed enumerated set for consistency and to enforce known semantics. User-defined relationship types could be powerful but might lead to chaos or misuse (and complicate UI). We can revisit this if needed by advanced users, but initially, we'll keep types to those we implement logic for.

Implications

- **Integrity Constraints:** We must ensure that edge endpoints (from/to block IDs) actually exist (no dangling edges). This implies when a block is deleted, all edges involving it should also be deleted or updated. We'll implement this in our block deletion mutation: it will query and remove associated edges to maintain consistency. Convex doesn't automatically cascade deletes, so we handle it in code.
- **Preventing Duplicates and Cycles:** The system should likely prevent identical duplicate edges (two edges with same from, to, type) unless there's a rationale (maybe different users want to mark support, but we can handle that via a single edge and maybe a separate vote count if needed). We will enforce uniqueness of (from, to, type) per edge either in code or by composite key logic. Cycles in the graph are possible (especially with semantic links). While cycles aren't inherently "wrong" (e.g., two documents referencing each other), they can cause infinite loops in naive graph traversal. We will need to be cautious in queries/traversals to avoid unbounded recursion. For example, a "contains" hierarchy should ideally be acyclic (a document can't contain itself); we will enforce that by design (no way to make an edge from a block back to one of its parents due to type constraints or checking).
- **Graph Query Complexity:** Convex queries are JavaScript functions that can query the tables. A single query can fetch direct edges easily, but multi-hop traversal (e.g., find all blocks that support something that refutes X) might require multiple queries or client-side logic. We might implement recursive traversal in code (server or client) when needed. This could be an area to optimize later or use an algorithm. For MVP, likely direct 1-hop or 2-hop queries suffice for features (like listing direct references, or showing one level of supports/opposes).
- **Permission Model Interaction:** Edges themselves likely need permission control as well. If a block is private (see ADR 6), should edges pointing to it be visible? We probably will treat edges as metadata that inherits visibility rules from blocks: i.e., if I cannot see Block B, I shouldn't see an edge "A supports B" in a way that leaks B's content. We'll enforce in queries: when listing edges, filter out any edge whose to or from is a block the user has no access to. Also, creating an edge likely requires permission to see both blocks (and perhaps write permission on the "from" block if we treat adding an outgoing link as editing that block's context). We will clarify these rules and implement checks in the edge-creation mutation.
- **UI Complexity:** Representing a graph to users means we need UI affordances. For MVP, we might keep it simple: e.g., under a claim block, list its supporting evidence blocks (pull via edges); or on a reference, show backlinks (what blocks reference this). This is doable by querying edges. A full graph visualization or traversal interface might come later. We should ensure the data model doesn't limit that – it doesn't, since it's a general graph.
- **Batch Operations:** Some user actions might create multiple edges at once. For example, if a user imports a markdown with footnotes, we might auto-create blocks for references and edges linking them. Our design can handle bulk insertion (just multiple mutations). But we must be mindful of consistency if part fails – likely wrap in a Convex function that either creates all required blocks and edges or aborts on error.
- **Edge Properties for Advanced Use:** We left room for properties on edges. In MVP, we might not use it much except maybe an order for "contains". But if needed, edges could hold attributes like a description or weight (e.g., a score representing how strong a support is). The model supports it without alteration, illustrating its flexibility.

Expansion Path

- **Additional Edge Types / Ontologies:** As the knowledge base grows, we may introduce more nuanced relationship types or even domain-specific ontologies. For instance, in a scientific context, we might add "extends" (for a study that extends prior work), "confirms", "disputes", etc. The ADR's model easily allows new types. We would also update UI and maybe color-code or iconify different edge types for user clarity.
- **Weighted or Annotated Edges:** In argument mapping, sometimes supports/opposes are weighted (strongly supports, weakly supports, etc.). We could add a weight or score field on edges, or represent that as separate types (e.g., "strongly*supports"). A more structured approach is to allow an edge to carry a small content payload – for example, an edge could have a justification text explaining \_why* Block A supports Block B. Our schema could accommodate an annotation field for that. In the future, edges themselves could even be elevated to full "block" status (like nodes) to allow rich annotations. But that complicates the model (an edge as a node in meta-graph). For now, simple properties would do.
- **Graph Visualization and Query:** We might implement a visual graph view of the knowledge. To do so efficiently, we might need to retrieve subgraphs (all nodes and edges within N hops of a given block). We may create specialized Convex functions that do depth-limited DFS/BFS to gather such subgraph data. If this becomes intensive, we could consider caching some graph structure or using an external search index for relationships. Expansion could also integrate with graph algorithms (shortest path, centrality) if we build analysis features. There is mention in research about computing metrics like centrality once a graph is built[[28]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=We%2520have%2520directed%2520graphs%2520where,the%2520shortest%2520routes%2520between%2520nodes), which could be interesting for identifying key blocks in the knowledge base.
- **Federation and Cross-Instance Edges:** If in future multiple instances of the system share knowledge, we might allow edges that connect blocks from different instances. This raises questions: how to uniquely identify a block globally (we might need a global ID scheme or URIs)? Possibly using something like Atproto/ActivityPub to reference remote content. We could extend edges with fields to hold an external reference (like toExternal: {instance: URL, id: externalId}). Initially out of scope, but the model could be extended to support "external edges" pointing outside the local Convex database. This would be key in a federated knowledge network where, say, a claim in one server is supported by evidence in another.
- **Automated Edge Creation (AI-driven):** With the introduction of AI agents (Mastra, etc.), we foresee tools that automatically suggest or even create edges. For example, an AI reading a block might suggest "This block contradicts Block Z" and propose a refutes edge. Or auto-citation: an agent finds a relevant source and creates a references edge. Our system can accommodate this since edges are just data – an agent with appropriate permissions could call addEdge. We might need to mark such edges as AI-suggested (maybe via a property or a specific subtype) so users can trust or verify them. This is an expansion that uses our core architecture to deepen the knowledge graph intelligently.
- **Hierarchical or Compound Edge Types:** We might eventually introduce hierarchy in relationships (some types might be sub-types of others). For example, a "supports" edge could be broken into "empirical_support" vs "theoretical_support". Managing such taxonomy might require either convention in type naming or an explicit schema. If needed, we could add an ontology table or use prefixes in type (like support:empirical). This would allow more complex reasoning. This is speculative, and we'd approach it when there's a clear use case.
- **Edge Life-cycle and Versioning:** Currently, edges are relatively static links. In the future, edges themselves might change or be contested (maybe one user asserts a support link and another disagrees). We could version edges like we do blocks – but that adds a layer of complexity (like an edge existence might have a history of being added/removed). For now, we assume edges are either present or not, but expansion could consider tracking when edges were introduced or removed in the knowledge evolution. This would align with an even richer knowledge management practice (like "history of how ideas became connected").

ADR 5: File Storage and Access Control (Convex File Storage + Files Control)

Context

The knowledge system will include **file attachments** – images, PDFs, data files, etc. We need to allow users to upload files and embed or link them in blocks. Key requirements are: handling potentially large binary files, efficient delivery (possibly via CDN), and **access control** so that private files aren't accessible to unauthorized users. Since we're using Convex as our backend, we aim to use its provided capabilities for file storage to streamline development. Convex offers an integrated file storage solution and there is a Convex component called _Files Control_ that provides a higher-level API for secure file handling. We must decide how to integrate this for MVP, ensuring that uploading and retrieving files is seamless and secure in our app's context.

Decision

**Utilize Convex's built-in File Storage with the Convex Files Control component to manage uploads, downloads, and file permissions.** We will treat files as a special kind of content stored outside the main database (binary storage), referenced by IDs in our Block model. Key points of the implementation:

- We install and configure the @convex-dev/files (or the third-party convex-files-control) component in our Convex backend. This provides server functions for generating upload URLs, finalizing uploads, and creating download URLs with access checks[[29]](https://www.convex.dev/components/files-control%23:~:text=Convex%2520Files%2520Control)[[30]](https://www.convex.dev/components/files-control%23:~:text=*%2520Two,for%2520presigned%2520or%2520HTTP%2520uploads).
- **Uploading files:** We use a two-step upload process:
- The client requests an upload URL from Convex by calling a mutation (wrapping the component's generateUploadUrl). This returns a one-time **pre-signed URL** and an uploadToken[[31]](https://www.convex.dev/components/files-control%23:~:text=//%2520Client,).
- The client then uploads the file directly to Convex storage via HTTP PUT to that URL (bypassing our app server for the heavy binary transfer). Convex returns a storageId if successful[[32]](https://www.convex.dev/components/files-control%23:~:text=const%2520,json).
- The client calls a Convex mutation (wrapping finalizeUpload) with the uploadToken and storageId to confirm the upload[[33]](https://www.convex.dev/components/files-control%23:~:text=const%2520result%2520=%2520await%2520finalizeUpload\(,1000,). In our wrapper, at this point we create a Block of type "file" in our blocks table linking to this storageId. We also pass an initial access control list for the file: by default, the uploader's user ID is given access[[34]](https://www.convex.dev/components/files-control%23:~:text=const%2520,). The Files Control component handles associating that user's ID (subject) as an access key on the file record[[35]](https://www.convex.dev/components/files-control%23:~:text=const%2520result%2520=%2520await%2520ctx,).
- **Serving files:** We do not serve files through Next.js; instead, we will generate secure URLs for file access:
- For inline images in content, we'll need a public URL. The Files Control includes an optional HTTP router we can enable to serve files through a path (e.g. https://<convex-site>/files/download?token=XYZ). We will likely enable the download route[[36]](https://www.convex.dev/components/files-control%23:~:text=A%2520Convex%2520component%2520for%2520secure,plus%2520a%2520React%2520upload%2520hook)[[37]](https://www.convex.dev/components/files-control%23:~:text=//%2520convex/http.ts%2520import%2520,generated/api) and use Convex's built-in auth hook to ensure only authorized requests succeed[[38]](https://www.convex.dev/components/files-control%23:~:text=checkUploadRequest:%2520async%2520\(ctx\)%2520=,Type%2522:%2520%2522application/json%2522%2520%257D,%2520%257D\);)[[39]](https://www.convex.dev/components/files-control%23:~:text=//%2520Optional:%2520provide%2520accessKey%2520for,).
- Alternatively, we can use the buildDownloadUrl function to create time-limited URLs on demand[[40]](https://www.convex.dev/components/files-control%23:~:text=import%2520,control). For example, when displaying an image, the app can request a URL for it, which will embed a short-lived token. This way, even if someone finds the URL, it expires (and if requireAccessKey is on, it won't work without being logged in)[[41]](https://www.convex.dev/components/files-control%23:~:text=).
- **Access control:** Each file stored in Convex has an associated set of **access keys** (just like blocks will have ACL). By default, we use user IDs as access keys. The Files Control component provides mutations to add or remove access keys for a file[[42]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520normalized%2520,empty%2520value). We will integrate this with our block permissions (ADR 6) such that if a file block is shared with a user or made public, we call addAccessKey for that file's storageId for the appropriate user IDs or a special "public" key. Conversely, if permissions are revoked, we remove keys. This ensures that when a user tries to download a file, the component's hasAccessKey check can verify they're allowed[[43]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520normalized%2520,empty%2520value).
- **Embeds and thumbnails:** For user convenience, we'll support directly embedding images in content. We may use an <img> tag whose src is the secure URL from above. For PDFs or other files, we might show a link or allow downloading. We'll rely on the browser or a viewer for actual rendering. If necessary, we could generate image thumbnails, but that's beyond MVP (and could be done via a separate service).
- **Storage backend:** Convex storage by default uses its own backing store (likely S3 under the hood). The Files Control component also supports Cloudflare R2 as an option[[36]](https://www.convex.dev/components/files-control%23:~:text=A%2520Convex%2520component%2520for%2520secure,plus%2520a%2520React%2520upload%2520hook). For MVP, we can use Convex's default (quick to start). If we expect heavy file usage or want to reduce costs, we could configure R2. The decision here is to stick with Convex-managed storage, possibly switching to R2 in production if needed by just providing credentials (the component's API supports both seamlessly).
- **Cleanup and retention:** We will use the component's built-in cleanup functionality. It provides a cron job to delete expired files and unused upload tokens[[44]](https://www.convex.dev/components/files-control%23:~:text=Cleanup). We will schedule this (the example shows a Convex cron that runs hourly to clean expired uploads and grants)[[45]](https://www.convex.dev/components/files-control%23:~:text=Use%2520,it%2520in%2520a%2520cron%2520job). We'll ensure to set appropriate expiration for any one-time links or temporary data. By default, if we don't specify an expiresAt for a file, it might live indefinitely. We might not use expirations on the files themselves (unless we want auto-expiry), but we will on ephemeral download links.
- **File metadata:** We will store file metadata (filename, size, content type, etc.) in our own blocks table or a separate filesMeta table. The Files Control example suggests storing your own file record after finalizeUpload[[46]](https://www.convex.dev/components/files-control%23:~:text=). We'll do that: e.g., create a block of type "file" with fields like fileName, fileSize, contentType, and the storageId. This allows us to display file info in the UI and search it if needed. The storage itself is opaque (just an ID for retrieval).
- **Large file considerations:** The presigned upload approach means files don't go through our server, which is good for large files. Convex likely has an upper limit per file (maybe ~5GB or so; not documented here). We will enforce a reasonable size limit on uploads at the app level (maybe a few hundred MB for MVP, configurable). If needed, we could split files or compress images on client side, but not initially.

Rationale

Using Convex's file storage and Files Control component is a pragmatic choice: - **Integrated Security:** The Files Control module is designed to enforce access control on file URLs, preventing unauthorized downloads[[36]](https://www.convex.dev/components/files-control%23:~:text=A%2520Convex%2520component%2520for%2520secure,plus%2520a%2520React%2520upload%2520hook)[[47]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520not%2520placed,consumeDownloadGrantForUrl). It generates one-time tokens and handles expiration. This saves us from implementing our own signing mechanism. For example, we can easily create a password-protected file link or a link that only works N times via the provided API if needed[[48]](https://www.convex.dev/components/files-control%23:~:text=Set%2520,to%2520return%2520an%2520access%2520key)[[49]](https://www.convex.dev/components/files-control%23:~:text=const%2520grant%2520=%2520await%2520ctx,passphrase%2522%2520%257D,). - **Development Speed:** It's much faster to leverage this existing component than to set up our own S3 and write lambdas to sign URLs, etc. The component even gives us React hooks for upload which simplifies the frontend integration[[50]](https://www.convex.dev/components/files-control%23:~:text=React%2520hook)[[51]](https://www.convex.dev/components/files-control%23:~:text=//%2520Presigned%2520await%2520uploadFile\(,). - **Consistency with Convex:** By keeping file handling within Convex's ecosystem, we maintain a single source-of-truth for both data and files. Convex's reactivity might even let us react to file metadata changes (though files themselves are static content). It also means deployment and environment config is simpler (no separate file server). - **Scalability and CDN:** Convex's approach (especially if using R2 or their default) likely delivers files via a CDN or at least efficiently. The \*.convex.site URLs are optimized for delivery. If using R2, Cloudflare's CDN is automatically in play. This means good performance for users fetching files, without us manually configuring a CDN. - **File Lifecycle Management:** The component's inclusion of cleanup for expired files and the ability to set an expiry on each file is valuable. For example, if we allow users to upload intermediate files or image previews that should auto-delete, it's straightforward. Also, if a user deletes a file block, we should consider removing the file from storage to save space. The component's deleteFile (via cleanUp.cleanupExpired or direct calls) can be used. - **Comparison to alternatives:** Using something like AWS S3 directly would require setting up signed URL generation (either via AWS SDK or cloud function), managing bucket permissions, and a separate credential set. The Convex component wraps all that, and using it keeps our stack homogeneous. Also, since Convex is already part of our app, we avoid cross-service complexity (like needing a separate backend just for file upload logic or CORS issues with S3). - **Example and Precedent:** The Files Control component is relatively new but appears to be a recommended approach by Convex (since it's listed as a Convex component). The feature set covers our needs: presigned upload, secure download, access key management[[36]](https://www.convex.dev/components/files-control%23:~:text=A%2520Convex%2520component%2520for%2520secure,plus%2520a%2520React%2520upload%2520hook)[[52]](https://www.convex.dev/components/files-control%23:~:text=Access%2520control%2520&%2520queries). Adopting it follows the "convention" for Convex apps dealing with files, reducing risk.

Alternatives Considered

- **Direct File Embedding:** One idea is to store small files (like images) directly in the Convex database as binary data (e.g., base64 in a block). This was rejected because it would bloat the database and slow queries. Convex's file storage is meant for binaries; using it is more efficient than embedding in blocks.
- **External Storage (S3/GCS) without Convex integration:** We could have used AWS S3 or Google Cloud Storage directly, writing our own backend endpoints to sign URLs. While feasible, it means more boilerplate and ensuring those endpoints are secure. We also lose the tight integration with Convex's auth. We decided the slight vendor lock-in to Convex's file system is acceptable for MVP given the time saved.
- **Serving via Next.js (hosting files on Vercel):** For small media, we could import them into Next.js public folder or use Vercel's built-in storage for static assets. That doesn't work for user uploads, since those are dynamic and would require rebuilding the app or an API to serve. Vercel has no built-in binary store for user uploads – one would still need to push to an external store. So this was not a real solution.
- **Peer-to-peer or IPFS:** Unorthodox but considered given "knowledge system" vibe – using IPFS or similar to store files decentralized. Not appropriate for MVP due to complexity and lack of fine-grained access control (everything on IPFS is public). We need straightforward, private file storage, which Convex provides.
- **No File Support:** Theoretically we could launch MVP without file attachments. But that severely limits the utility (users often need to add images or PDFs). We judged file support as a necessary feature of the knowledge platform MVP.

Implications

- **Auth Integration:** We will use Convex Auth (likely using OAuth providers or email login) to identify users. The file access keys rely on the user's identity.subject (unique user ID)[[53]](https://www.convex.dev/components/files-control%23:~:text=handler:%2520async%2520\(ctx,%2520args\)%2520=,Unauthorized)[[54]](https://www.convex.dev/components/files-control%23:~:text=,Unauthorized). We must ensure every file operation is tied to an authenticated user. So we'll require login for uploads and downloads (for private files). Public files can be handled by using shareable links that bypass auth but those we'll only generate intentionally. We need to be careful to never leak a raw storage URL without a token.
- **App Size / Bandwidth:** Handling files means potentially large data transferring. We need to keep an eye on cost (Convex or R2 bandwidth costs, Vercel edge function usage if any). If users upload very large files frequently, it could be an issue. We might impose file size limits and encourage compression (like large videos might not be allowed).
- **Content Security:** Our system doesn't inherently virus-scan or moderate file content. For internal usage this might be okay; for a public platform, might need to integrate virus scanning or content moderation on images. For now, we assume trust among users or scope that out of MVP.
- **Backup and Data Persistence:** Files stored in Convex presumably are persisted redundantly by the service, but we should verify. If we ever need to migrate away from Convex storage, we'd have to copy files out using the storageIds (maybe via an export script). Slight lock-in: files are not as easily migrated as data in code (though we can always write a script to fetch all and upload elsewhere).
- **User Experience:** Uploading via presigned URLs can be a bit less straightforward than a direct form post, but our use of the React hook (useUploadFile) should hide complexity[[50]](https://www.convex.dev/components/files-control%23:~:text=React%2520hook)[[51]](https://www.convex.dev/components/files-control%23:~:text=//%2520Presigned%2520await%2520uploadFile\(,). We must ensure to provide progress feedback in UI (the hook likely gives progress events or we can tap into the fetch progress). Also, when an upload finishes, we should handle showing the file block (perhaps the finalizeUpload mutation returns the new block's ID).
- **Filename and URL hygiene:** The download URLs we generate can include the original filename for user convenience (the buildDownloadUrl allows specifying a filename param for content-disposition[[55]](https://www.convex.dev/components/files-control%23:~:text=)). We should do that so downloads have correct names (e.g., "report.pdf" instead of a random ID). Also, our system should handle filename collisions or sanitization (the file component likely handles unsafe characters).
- **Managing File Access with Block Sharing:** We need to keep file ACL in sync with block ACL. If a user shares a document containing an image with someone, the image file must also become accessible. This implies that our share logic (ADR 6) on blocks will include: if making a block (or its parent doc) available to a user, iterate through any file blocks inside and call addAccessKey for those files as well. Conversely, if revoking access, remove keys. This coupling means more code but is necessary for security. Alternatively, for convenience, we might decide that _all files inherit the permissions of their parent block (document)_. We can enforce that by always giving any user who can see a file block the access key. A future optimization might be grouping files by project or using tenant IDs as access keys (the component allows arbitrary strings as keys, not just user IDs[[56]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520normalized%2520,empty%2520value)). For now, user IDs suffice.
- **Expiring Links vs Direct Auth Check:** The Files Control can either use expiring tokens in URL or require that the request include an auth token (if requireAccessKey is true)[[41]](https://www.convex.dev/components/files-control%23:~:text=). If we use the built-in HTTP route, since it's on the Convex domain, we might leverage the user's Convex auth session cookie (if exists) to authenticate download requests. This is convenient (no need to generate tokens for internal use). However, in embedded images (on a webpage), using a session cookie might not work due to cross-origin issues (our app domain vs convex domain). So using tokenized URLs might be simpler for images. We will likely generate short-lived URLs for images so they can be fetched without user cookie (the token in URL suffices). This is a trade-off: token in URL can be copied and used by others until expiry. But if short (say 5 minutes) and continuously refreshed (like re-fetch on access), it's reasonably secure.
- **UX of Public Files:** If we eventually allow truly public blocks, their attached files need public access. The component supports generating a **shareable link** (which essentially marks the file as accessible without auth)[[57]](https://www.convex.dev/components/files-control%23:~:text=Shareable%2520links). We would use shareableLink: true on a download grant to serve publicly. This means if a block is published publicly, we can mark its files as shareable. We'll document this and ensure to only do so when intended.

Expansion Path

- **Image Thumbnails and Processing:** Down the line, we might integrate an image processing step. For instance, generating a smaller thumbnail for large images to display in previews. Convex doesn't natively process images, but we could integrate a third-party (or possibly use Vercel Edge Functions) for that. Alternatively, use the **AI/ML** capabilities: e.g., if we had an AI pipeline, generate text from images or vice versa. This is beyond MVP. If needed, we could store multiple versions of an image (original and thumbnail) under different storageIds.
- **Video and Audio Handling:** If the system later supports multimedia (videos, audio), we might offload to specialized services for encoding/streaming (like Mux or Cloudinary). But in architecture terms, we'd treat it similarly: store file and maybe store a link to a streaming URL. Integration with those could be a future ADR.
- **Large Data Files and External Integration:** Researchers might upload dataset files. If these get very large, Convex's storage might not be cost-effective. We might integrate with user's own cloud or institutional storage (like link to Google Drive or S3 bucket). That complicates unified access control, but we could store reference links instead of the actual file. In expansion, we might allow blocks of type "external_file" that just hold a URL (and maybe an authentication token if needed). The decision for MVP is to actually store files, but we remain open to linking instead for huge content.
- **Public File Repository:** If we open parts of the knowledge base publicly, we might want to serve certain files more directly or via static hosting. Perhaps we could offload all public images to a static site/CDN for performance. This could be an automated process: if a block is marked public, copy its file to a public bucket and use a public URL. That way, we don't strain Convex for heavy public traffic. The architecture can support this: we'd maintain the storageId for internal use and a publicURL field if exported. This is optimization for scale.
- **File Versioning:** Currently, if a user updates a file (e.g., replaces an image), we'd likely treat it as a new file with a new block, and maybe mark the old one deleted. We might consider allowing version history on files too (like older attachments). For MVP, not needed. In the future, an ADR could specify that updating a file block keeps the same block id but points to a new storageId, and we keep the old storageId in history (or as an older version).
- **Encryption:** For highly sensitive data, server-side checks might not be enough; end-to-end encryption could be desired (only clients with keys can decrypt files). This is non-trivial and not in scope for MVP, but our architecture could be extended: we could encrypt files on upload and store the key in a secure place per authorized user. This would impact how downloads work (need client-side decrypt). Not likely needed unless dealing with very private research data, but worth mentioning as a possible future enhancement if security requirements heighten.
- **Integration with Workflow/Agents:** If we have background processes or agents (see ADR 8) that produce files (like an AI that generates a report PDF, or a workflow that collects data and stores a CSV), we can integrate them by having those processes use the same file storage mechanism. Convex scheduling could generate a file and use the same finalizeUpload with an appropriate access key. Our consistent approach to file storage means even automated content goes through the same pipelines, simplifying maintenance.
- **User Quotas and Cleanup:** As usage grows, we might implement user-level storage quotas to prevent abuse (especially if this becomes multi-tenant or public). The architecture can handle it by checking a user's total file sizes before allowing new upload. Also, a more aggressive cleanup for unused files (if a file block is deleted, delete the file content immediately rather than just marking expired). We'd likely add a Convex job to remove or archive file content when no referencing block exists.
- **Content Delivery Optimization:** For frequently accessed files, consider a caching strategy. Perhaps instruct Convex's storage to cache certain files or use a CDN. If using R2, Cloudflare caching is built-in. If heavy usage, we might move extremely common public files to a known CDN or even embed them directly if small (for example, icons or small images could be Base64 in the page – but larger ones no).
- **Multi-file Uploads and Folder Structures:** Currently, each file is separate. In future, if users want to upload a batch of files or maintain a folder-like organization, we might introduce a concept of a "Folder" block (which contains file blocks via edges). This doesn't change file storage but adds UI grouping. The architecture of edges (contains edges) already can represent that. So expansion might be more about UI and minor metadata (like a folder block).
- **Search in Files:** A future feature could be searching within PDFs or text files. That would require extracting text content (via an OCR or text extraction pipeline) and indexing it (maybe in Convex or an external search index). This is an advanced expansion. If done, we might store the extracted text in a hidden field of the file block for indexing. We note this as it intersects file storage with content knowledge.

ADR 6: Permissions and Embargo Model (Block-level ACL with Time Locks)

Context

We need a **permissions model** to control who can view or edit content, as the system may contain private or sensitive knowledge. Additionally, the notion of an "embargo" is introduced – content that should remain hidden or read-restricted until a certain time (common in research or media to prevent premature disclosure). Our MVP is not federated, so we can assume a single tenant or organization context for now, but we still need to differentiate access between users (and possibly roles) within that context. For example, certain blocks might be private to their author, others shared with a team, and others fully public. We want to enforce these rules at the block level, since our content model is block-granular (ADR 1). Embargo suggests a **time-based access control** aspect, where a block might automatically become visible after a timestamp[[58]](https://jumpcloud.com/it-index/what-is-time-based-access-control%23:~:text=Time,based%2520on%2520predefined%2520time%2520constraints).

Decision

**Implement block-level Access Control Lists (ACLs) combined with optional time-based embargo restrictions.** Concretely:

- Every Block in the blocks table gets a permissions field, likely called acl or access. For MVP, this can be a simple structure: for example, access: { readers: string[], writers: string[], public: boolean, embargoUntil?: Date }. We can simplify further to just a set of allowed user IDs and a flag for public visibility. We'll also include an owner for each block (usually the creator's user ID) which always has full access.
- **Read Access** — To retrieve or subscribe to a block (and its content/edges), the requesting user must be either listed in the block's allowed list or the block is marked public. Otherwise, the Convex query will not return that block (or the function will throw an authorization error). This check will be built into all our query functions that fetch blocks. Convex doesn't automatically enforce row-level security, so our code will do it.
- **Write Access** — Similarly, modifications (edit text, add child, etc.) require the user to be either the owner or explicitly in a writers list (if we maintain a separate one) or in a role that permits editing. For MVP, we might treat write access as having the same list as read (i.e., shared users can both read and edit by default). We can refine that with separate lists if needed.
- **Public Blocks** — A block marked as public: true is viewable by anyone, including unauthenticated users (if the app supports anonymous access). For now, we might restrict to authenticated users anyway, but public could bypass user-specific check. In the future, public means we might expose it without login (e.g., via a public URL).
- **Inheritance** — We decide that permission is primarily per block. However, to ease management, blocks that are part of a larger document or project should inherit or be automatically assigned the parent's permissions by default. For example, if you share a document block with Alice, all its child blocks should become visible to Alice as well. We will implement this by a propagation mechanism: when setting ACL on a parent, cascade it to descendants (or at least ensure our queries treat possession of parent access as implying child access). A simpler approach: always check up the containment chain – if a user has access to an ancestor block, allow access to the block. This requires queries to possibly join up the tree. Instead, we might explicitly propagate for performance. MVP can assume that content is primarily shared at a document (top-block) granularity to simplify (we can enforce that sharing toggles on top-level blocks and auto-applies downward).
- **Embargo** — We add an embargoUntil timestamp on blocks that need it (only certain types, e.g., a research result block that shouldn't be public until a publish date). The semantics:
- If embargoUntil is a future time, then until that time passes, treat the block as if it were not shared publicly (and possibly even restrict it from certain collaborators if needed). We likely implement embargo as an override: e.g., a block could be marked public but with a future embargo – meaning it will _become_ public at that time, but until then only visible to its owners or a small group.
- We could also use embargo for internal staging: maybe share with collaborators but embargo from wider group.
- We will enforce embargo in the read check: if now < embargoUntil and the requesting user is not in a special allow list (perhaps the owner or explicitly allowed even during embargo), we deny access even if it would otherwise be allowed by ACL. Essentially embargo is a time lock that trumps other settings.
- Once the embargo time passes, the block's status flips to normal (for example, if it was intended to be public, it becomes accessible to all logged-out or all users).
- **Administration** — Possibly, designate some users as admins who can override permissions. For MVP, we can keep it simple: maybe all users in the system can create and share their blocks, no strict admin role. If needed, we treat the first user or certain emails as admin who can see all (but that's optional).
- **Implementation details**:
- On block creation, we set default ACL: usually owner has it in allowed list, and maybe no one else (private by default). If the block is created inside a shared document, we'll immediately assign the parent's ACL to it.
- We'll provide mutations to share a block: e.g., shareBlock(blockId, userId) which adds that user to the block's readers (and maybe writers). For bulk sharing (like share a whole doc), it will traverse children and add them too (unless we rely on parent inheritance logic).
- Another mutation might be setPublic(blockId, true/false) to publish or unpublish a block.
- For embargo, a mutation setEmbargo(blockId, timestamp) to schedule releasing. Possibly also liftEmbargoNow if needed.
- We will utilize Convex's cron jobs for embargo: e.g., a cron can run every hour or minute to find blocks whose embargoUntil has just passed and flip their public flag or notify owners. Alternatively, we can simply check at read time; but for convenience, we might proactively change a block's state at embargo expiry. For example, if a block was embargoed but intended public, we might set public:true at the moment embargo passes. This can be done with a scheduled job if precision is needed (Convex scheduling can do hourly or daily checks[[59]](https://www.convex.dev/components/files-control%23:~:text=import%2520,generated/api), but not sure about to-the-minute scheduling without custom logic; possibly we schedule an individual job via workflow, if supported).
- The Files Access (ADR 5) will tie in: when a block's ACL changes, we call file permission updates accordingly. Also, if a block is embargoed, its files should effectively be embargoed (not accessible via shareable link until time passes). We can handle that by not generating public links for them until embargo lifts.
- We consider **Time-limited access** (embargo is one type, but maybe we also consider expiry of access). For MVP, embargo covers the main time factor (start time of access). We won't implement auto-expiry of access (like "user can see this until date X") unless needed. But the infrastructure could handle it similarly by a cron removing user from ACL after a time.

Rationale

- **Fine-grained control:** Block-level ACL means any piece of information can be separately secured. This matches our block model: if you embed a confidential note inside a public document, you could technically restrict that block. (Though for UX, we might simply not allow that because it complicates reading a doc – better to keep blocks in a doc uniform. But the capability is there if needed.)
- **Simplicity for MVP:** We favor a straightforward allow-list approach over complex role-based permissions. Each block carries an explicit list of who can access. This is easy to check and reason about. It's analogous to Google Docs style sharing – each doc (block) has a list of permitted users or a public flag. We don't have to implement separate group objects or roles at MVP (everyone is basically an individual or "all").
- **Support for Team Collaboration:** Even in MVP single-tenant, not everyone sees everything. Researchers might have personal notes (private blocks) and team projects (shared blocks). The ACL model supports that easily. It's also flexible enough that if we later introduce groups, we could treat a group as an "identity" in the allowed list (just an entry that we resolve to multiple users).
- **Embargo as Time-Based Access Control:** Time-based access control (TBAC) is recognized as useful for enforcing release times[[58]](https://jumpcloud.com/it-index/what-is-time-based-access-control%23:~:text=Time,based%2520on%2520predefined%2520time%2520constraints). By including embargoUntil, we automate what might otherwise be manual. For example, if a paper is under embargo until publication date, the user could mark it so. The system will then not show it to others (or not make it public) until that date passes, at which point it can automatically appear. This prevents human error (accidentally sharing too soon) and saves the user from waking up at midnight to flip a switch – the system does it.
- **Convex fit:** Convex's security model is mostly in userland – we write the checks. It gives us full flexibility to implement ACL our way. We prefer that over something like attaching policies in an external service. Within our Convex functions, ctx.auth.getUserIdentity() gives us the user performing the request[[53]](https://www.convex.dev/components/files-control%23:~:text=handler:%2520async%2520\(ctx,%2520args\)%2520=,Unauthorized). We then simply filter or throw if not in allowed list. This straightforward approach means minimal performance overhead and complexity (just checking a list).
- **Alternatives complexity vs benefit:** More elaborate systems (like full Role-Based Access Control with roles and permissions on operations) seemed overkill. Our approach covers the basics: who can read, who can write, and time gating. It's similar to how Notion or Coda handle sharing (block or page level sharing to people).
- **Ensuring minimal friction:** We want users to be able to share content easily without needing an admin to configure roles. So users can directly add someone's email (user ID) to a block's ACL if they have rights. This decentralized sharing improves collaboration. Meanwhile, having an embargo option built in will appeal to use cases like pre-publishing research or time-sensitive announcements.

Alternatives Considered

- **Document-Level Permissions Only:** We thought about simplifying to only top-level documents have ACL, and all their child blocks inherit that fully (no per-block differences). This is actually likely how users will commonly use it (share entire doc or not). Enforcing that rule would simplify enforcement (just check top-level). However, it removes flexibility to have, say, a private section in an otherwise shared doc. We decided to keep the possibility of per-block ACL for now, but UI may discourage mixing private/public in one doc to avoid confusion.
- **Role-Based Access:** Using roles (like "admin", "member", "viewer", etc.) and assigning roles to blocks. This could be a layer above ACL (like instead of listing user IDs, list roles and have a global user-role mapping). For MVP, we skip this complexity because all users are basically equal collaborators, and if needed, an "admin" can simply be a user who has access to all by being added everywhere (or by virtue of being an owner of all top docs).
- **Capability Links (Shareable links with secret tokens):** We considered implementing sharing via secret URLs instead of explicit user lists (like how you share a Google Doc with "anyone with the link"). This means generating a token and not requiring login for that token. It's a useful feature (especially for public sharing). We'll partially get this via the "public" flag (which essentially is like a global link) or by generating invite links manually out-of-band. But MVP will likely focus on user identity-based sharing. The "public" flag could be seen as a special case of a token known to everyone. We could also implement invite links by creating a token and storing it in block ACL as an accepted key (like a pseudo-user key). Due to time, likely skip in MVP beyond just marking public or not.
- **Encryption-based security:** Another approach is to encrypt content for certain users so even the server cannot read it without keys. This is zero-trust approach. We did not choose this because it complicates collaboration (keys distribution) and we trust our server environment for an internal MVP. If security demands increased, we could consider end-to-end encryption for certain blocks, but not now.
- **No Embargo, just manual control:** The simplest approach to embargo is "don't show the block until the user manually flips it". But that relies on humans and might fail. Automating it is a small effort and high reward for correctness. So we included it. Another alternative is to implement embargo as a separate "Embargo" object tracking a set of blocks under embargo and releasing them – that's unnecessary overhead; a timestamp on block is enough.

Implications

- **User Interface & Usability:** We need to expose sharing controls in the UI. This might be as simple as a "Share" dialog on a document where you enter emails to share with, similar to Google Docs. We'll have to implement that on frontend, calling our Convex functions. If multiple blocks (like all children) need updating, it could be slow if many – but presumably manageable (or we enforce at top-level).
- **State Management:** Each block's ACL is now part of its state. We might want to keep ACL info out of the main collaboration editing stream to avoid frequent updates (for example, we wouldn't want an ACL change to conflict with text edits). We'll likely handle ACL updates via separate mutations outside the ProseMirror steps. It's a rarely changed metadata, so no issue.
- **Performance of Checks:** Checking membership in a list of user IDs is trivial unless the list is huge. Typically, a block might be shared with a handful or maybe dozens of users, not thousands (except public which we treat as a flag). So performance is fine. For public, we can either interpret public:true as "no need to check list" or we could insert a special marker like userId "\*" in readers list to denote public. We just need to handle it consistently. Probably a boolean is simpler.
- **Data Redundancy:** If we propagate ACL to child blocks, we store duplicate lists on many blocks. This is redundant, but ensures fast permission checks (just check the block itself). The risk is inconsistency (child's ACL not updated when parent's is). We must manage that carefully in share functions (e.g., always use a transaction or multiple mutation calls to update all children). Alternatively, not store on child at all and always refer to parent – but that breaks if a block is moved to another doc. We lean towards storing on each block for self-contained info, accepting the complexity of keeping it in sync (maybe easier if share is mostly at doc level).
- **Convex Auth and Identity** — We rely on Convex's user identities (likely an OAuth subject string or Convex user Id) to identify users in ACLs. We should use a stable unique ID (like Auth0's sub or GitHub ID string) rather than email (emails can change). Convex Auth gives us identity.subject which is stable[[53]](https://www.convex.dev/components/files-control%23:~:text=handler:%2520async%2520\(ctx,%2520args\)%2520=,Unauthorized). We will use that as the key in ACL lists.
- **Future Federation impact:** In a single instance, user IDs are unique. In a federated future, identity would be more complex (like user@instance). If we federate, ACL might need to reference external users. That's a future problem – but maybe worth noting in expansion path.
- **Security** — We must ensure no ACL bypass. That means auditing all queries and mutations to enforce checks. E.g., a mutation that edits a block should verify ctx.auth user is authorized. A query that finds blocks via edges should filter out ones you can't see. We have to be vigilant to not accidentally leak data (like an edge query returning an ID of a hidden block is already a leak). Possibly in our edges queries, we should join with block access: e.g., when listing references to a block, filter out any referencing block the user can't read, and also if the target is the hidden block, probably the user wouldn't have asked in first place. It's tricky but doable – essentially any path that could reveal a block's existence or content requires permission. We will write tests to ensure that.
- **Embargo handling complexity:** If a block is under embargo and public flag is true, our code will need to hide it until time. We also should consider UI – maybe show it to owner with a label "embargoed until X". After time, automatically flip some state (which might require a refresh or a push). If we implement a cron job to lift embargo, we could even notify users or automatically refresh the content for viewers. Without that, a viewer might still see nothing until they refresh after time passes. Possibly acceptable. Maybe simpler: for a public embargoed block, we treat it as not existing to others until time, then it appears. That could surprise users, but that's expected by definition of embargo.
- **Temporal edge cases:** Timezones and exact timing – we store embargoUntil in UTC in DB, compare with Date.now() in server which is also UTC epoch. Minor issue: if we only run a cron hourly, an embargo might lift up to 59 minutes late if we rely on cron. Better to check on access in addition. Perhaps combine: use a cron (or ideally Convex "scheduling" if supports scheduling a job at an exact time, not sure if available) to flip a isEmbargoLifted or so, but always also double-check time in read logic. That way, worst-case scenario if cron hasn't run yet, a read function seeing now past embargo will allow access anyway even if public:true wasn't set yet.
- **Testing scenario:** We should test that a user who isn't allowed cannot retrieve block via all possible methods (direct get, through edges, through search if we have one, etc.), and that embargo works (e.g., a user tries to access an embargoed public block before time and is denied).
- **Interactions with Collaboration:** If a block is collaborative (shared editing), all editors likely have write permission. We need to handle concurrent edits with permissions – ideally ProseMirror collab only starts if you have write. We should enforce at the mutation level that only writers can submit steps. If a user is read-only and somehow tries to submit an edit, the function will reject it. Also, in UI we might make the editor read-only for them. So incorporate that state in the front-end (e.g., if not in writers, don't allow typing).
- **Propagation on Copy/Move:** If a block is moved (say cut-paste into another document), what happens to its ACL? Possibly it should inherit new parent's ACL after move (especially if moved from private to a shared doc or vice versa). We should address that: our move mutation should recompute or prompt user to decide. Simplest: if you move a block under a new parent, we could automatically set its ACL to match the new parent (assuming user moving has rights to do so). If that results in losing some old permissions, maybe warn. MVP likely won't have complex move operations exposed to user, but our system should handle it if done.

Expansion Path

- **Group Sharing:** Add concept of groups or teams to simplify sharing to many people. For example, instead of listing 30 users on a block, have a group "Research Team" that contains those 30, and just share to that group. Implementation: group could be a Convex object with id and list of user ids. ACL could allow entries that are group ids (distinguishable from user ids). The check function would then resolve group membership. This reduces duplicate lists and makes adding a new team member easier (just add to group). It's a logical next step if user base grows.
- **Roles and Permissions matrix:** Possibly define roles like "viewer" vs "editor". Currently, our ACL implicitly handles that (if we separated readers and writers). We could formalize roles: e.g., a "commenter" role could be introduced (if we have comment feature) or "owner" vs "co-editor". Right now owner is just the creator, not explicitly in lists (or could be included for clarity). Expanding roles would be about adding metadata to ACL entries or separate ACL lists per permission type. This can be done while preserving compatibility (just extend the structure).
- **Hierarchical Permissions:** If we want a structure where sharing a parent automatically and dynamically shares all children (and future children), we might implement an inheritance model. For instance, a flag on a block "inherits ACL from parent unless overridden". This would allow having sections in a doc that override (like mark one sub-block as extra restricted or open). This complexity could be toggled if needed. We'd then change our permission check to walk up until it finds a block with explicit ACL or hits top. For now, not doing this to avoid overhead on each check.
- **Public Publishing & Federation:** When we allow public access (no login), we need to deliver content possibly via a public endpoint (maybe a Next.js rendered page or a Convex HTTP endpoint). We'll have to ensure those follow the public flags. For federation (if we integrate with something like ActivityPub to broadcast public content), the permission model should align that "public" means world-readable. Embargo in that case might translate to scheduling the federation broadcast at a later time rather than immediate. That would be an interesting expansion – connecting our ACL flags to federation controls.
- **Audit Logs:** In enterprise contexts, one might want a log of who accessed what sensitive block and when. We might later add logging in our queries (though that can be heavy) or better, a dedicated system log of accesses. At least, logging changes to permissions is important (who shared what with whom). We could record ACL changes in an audit collection. This is an expansion for security-conscious deployments.
- **Temporary Access Links:** Provide a feature to generate a time-limited link to share with someone not in the system (like "share with external collaborator for 7 days"). This would involve creating a one-time token that maps to the block and storing an entry in ACL as a token with expiry. Our architecture could incorporate that by treating token as a pseudo-user in ACL and handling in request (like if a request provides a token, match it). Convex HTTP endpoints could be used to consume such tokens. This is a non-trivial feature but useful. It extends the permission model beyond user identity to include bearer tokens.
- **Future Integration with Mastra (AI agents):** If we add AI agents that can read or write blocks (co-scientist assistants), we might need to give them special permissions. One approach: treat an agent as a special user identity (with its own ID and appropriate ACL entries). Or allow the agent to act with a user's permission (like it runs under a user's context). For fine security, possibly each agent could be whitelisted on certain data. We'll consider adding an "agent" flag in ACL or simply include them as users. Our current model could already include an agent's userId in the allowed list.
- **Multi-Tenancy Prep:** If in the future we support multiple distinct organizations on one Convex instance, we'd add a "workspace" or "tenant" field on blocks and edges, and ensure ACL primarily doesn't allow cross-tenant shares (or if does, very explicit). That might be beyond MVP, but it's considered: the permission checks would then also isolate by tenant. We'd likely include tenant context in every query. For now, assume one tenant or all users potentially know each other.
- **Granular Time Controls:** We could expand embargo to more general time-based rules (like only accessible during certain hours or after a certain event). For example, a block might be visible for 24 hours after first view (like expiring info). Those are specialized scenarios (like timed exam content). If needed, we could implement with similar approach: store a release and/or expiry timestamp. In general, our architecture supports an expiresAt similar to embargoUntil (just reversed: after that time, revoke access). Actually, we could implement expiry by scheduling a removal of that user from ACL at time X. Or by storing an expiry on the ACL entry itself. Not doing now, but expansion could borrow ideas from how Files Control handles expiry for download grants[[60]](https://www.convex.dev/components/files-control%23:~:text=storageId,%2520maxUses:%25201,%2520expiresAt:%2520Date,).
- **UI/UX improvements:** Eventually, we might implement a permission management UI that shows all users who have access, allows setting view vs edit rights, transferring ownership, etc. That's product work but based on the data we store.
- **Policy Templates:** In some environments, we might have a policy like "all blocks created under Project X should default share with group Y". We could implement templates or rules at a higher level. This would be an extension to auto-set ACL on new content based on location or metadata. The architecture can accommodate by hooking into creation flows or having a config that our functions read. For example, if block is under a "ProjectX" parent, auto-add groupX to its ACL on creation.

ADR 7: Presence and Cursors (Realtime User Awareness)

Context

In a collaborative application, it's important to provide **user presence awareness** — seeing who else is online, viewing or editing the same content, and where their cursor or selection is. This not only helps avoid conflicts (e.g. you might avoid editing where someone else is typing) but also adds a sense of collaboration and social connection[[61]](https://stack.convex.dev/presence-with-convex%23:~:text=Presence,%2520as%2520we%25E2%2580%2599ll%2520use%2520the,in%2520a%2520Google%2520Doc,%2520etc). Features typically include: a list or avatars of users currently viewing/editing a document, indicators if someone is typing, and live cursors or highlights showing each user's position in the text. We want to implement this in our MVP to enhance the multi-user experience. Convex, our backend, has real-time capabilities but presence data has different characteristics: it's ephemeral (no need to persist long-term every cursor move) and high-frequency (cursor moves can be many per second). We need an approach that is efficient and doesn't overload the system, while still giving a smooth experience.

Decision

**Implement a Convex-based presence system tracking each user's activity per document, with periodic heartbeats and lightweight cursor data, and display this info with minimal latency.** The main components are:

- **Presence Table:** We will create a Convex table, say presence, with entries keyed by (documentId, userId). Each entry contains fields like:
- userId (or session ID if multiple sessions per user),
- documentId (or could be blockId if presence is tracked at any block level; likely we track at top-level doc for editing context),
- lastActive: number (timestamp of last heartbeat or action),
- cursorPos: object (could be a representation of their current cursor/selection in the document),
- status: string (e.g. "editing"/"online"/"idle", etc., or flags like isTyping: boolean).
- **Join/Leave:** When a user opens a document or collaborative editor, we will add or update their presence entry for that doc. When they leave (navigate away or disconnect), we should remove or mark them offline after a timeout.
- **Heartbeat Mechanism:** Each client will send a heartbeat (via a Convex mutation) every few seconds (e.g. every 5 sec as a default) to update lastActive. This is to signal they are still present[[62]](https://stack.convex.dev/presence-with-convex%23:~:text=A%2520common%2520way%2520to%2520detect,resources%2520your%2520app%2520will%2520consume). We can optimize by piggybacking on other messages: for instance, if the user is actively editing (thus sending steps), that inherently updates presence, so separate heartbeat might be skipped during activity.
- **Reactive Query:** We will have a Convex query function that, given a documentId, returns all presence records for that doc (perhaps filtering out ones that are offline beyond a threshold). Because Convex queries are reactive, all clients subscribed to this query will get updates whenever any presence record changes[[63]](https://stack.convex.dev/presence-with-convex%23:~:text=Reactivity). This means when someone new comes online or moves their cursor (if we update the table for cursor moves), others will know.
- **Cursors Handling:**
- For text cursors: We need to represent the position in a way others can map onto their copy of the document. If all clients are in sync via ProseMirror OT, they share a document version. We could store the cursor as a position (index) in the document. However, due to live edits, an index can become stale quickly unless updated after each remote step. A robust solution is to use ProseMirror's position mapping: when broadcasting a cursor, attach it to a specific version or transform it through operations. This can be complex. Simpler MVP approach: if two people type in different areas, positions shift mostly for one end. We might accept slight inaccuracy or update cursors lazily after an edit.
- We might integrate cursor with collab: for example, update cursor position in presence whenever the user's editor state changes (ProseMirror can provide a mapping from old to new positions after each step). We can also use strategies like storing a unique marker in the document for each cursor (some collab systems do this), but ProseMirror's collab plugin didn't natively include that.
- Given time, MVP might opt for a simplified approach: track _which block or section_ a user is focused on rather than exact character. Or just track cursor index without perfect adjustment – in moderate editing scenarios, it will usually be near correct. We note that **indexing cursor positions in presence data is complex** because of concurrent edits[[64]](https://stack.convex.dev/presence-with-convex%23:~:text=Text%2520cursors%2520are%2520an%2520important,it%2520at%2520that%2520for%2520now). We may limit exact cursors to when users are not actively editing.
- Alternatively, for MVP, we may omit live text cursors and just show user highlights (like colored selection) when they select something, using ProseMirror's collaborative editing awareness if available.
- For mouse cursors (if we wanted to show remote mouse pointers): that's quite high frequency and not essential for text editing. We will likely skip showing actual mouse position, focusing on text cursor/selection only. If we did, we'd not store every move in DB due to choppiness[[65]](https://stack.convex.dev/presence-with-convex%23:~:text=For%2520mouse%2520cursors,%2520the%2520challenge,where%2520the%2520cursor%2520is%2520moving) and ephemeral nature[[66]](https://stack.convex.dev/presence-with-convex%23:~:text=Sharing%2520mouse%2520cursor%2520positions%2520are,the%2520data%2520is%2520especially%2520ephemeral).
- **Frontend Display:** We'll show a "facepile" of user avatars on the document, possibly using the logic of grouping by online/offline and stable ordering (e.g., sort by join time to avoid shuffling)[[67]](https://stack.convex.dev/presence-with-convex%23:~:text=A%2520%25E2%2580%259Cfacepile%25E2%2580%259D%2520is%2520a%2520popular,in%2520mind%2520when%2520building%2520them). Each user in presence can be displayed with a colored cursor or name label at their text position if possible. If multiple users, each gets an assigned color. We can reuse presence data (like an entry could have a color field assigned when they join, or we just pick color by hash of userId).
- **Latency Considerations:** Presence updates (like typing status or cursor) can be frequent. We will **rate-limit presence updates** to avoid spamming. For example, update cursor position at most, say, 5 times per second or on idle stop (if someone is dragging cursor, we don't need intermediate points). The Convex presence article suggests using **single-flight** (dropping intermediate updates if one is already in flight)[[68]](https://stack.convex.dev/presence-with-convex%23:~:text=Presence%2520performance) to manage load. We can adopt that: essentially, if the user is moving the cursor rapidly, we batch updates or only send the latest.
- **Durability:** Some presence info we want ephemeral (cursor position doesn't need to be saved if everyone leaves). But some we might store longer: e.g., lastActive could be used to show "last online 5 minutes ago" if someone left. We will keep presence entries for some time after disconnect (or mark them offline). Possibly a separate table or reuse same but with status. Clean-up: We might have a cron that removes stale entries after, say, 24 hours of inactivity to keep table clean.
- **Typing indicator:** We can update a flag if user is typing vs just viewing. A simple approach: update status to "typing" when the user is actively entering text and set to "online" when not. We could detect typing by keypress events and then revert to normal if no typing for a couple seconds.

Rationale

- **Social presence improves user experience:** As noted, seeing collaborators in real-time gives a sense of connectedness and prevents feeling like you might overwrite each other blindly[[69]](https://stack.convex.dev/presence-with-convex%23:~:text=someone%2520is%2520composing%2520a%2520message,in%2520a%2520Google%2520Doc,%2520etc). It turns a static wiki into a lively workspace.
- **Convex is suited for presence:** Convex's real-time push and automatic invalidation means we can implement presence without setting up separate WebSocket channels manually. Just writing to a presence table and querying it uses Convex's reactive engine to broadcast changes to all in that room[[63]](https://stack.convex.dev/presence-with-convex%23:~:text=Reactivity). This is simpler than a custom pub-sub. Also, Convex can handle frequent small writes in memory and only push last state thanks to single-flight mechanism (which batches rapid table writes)[[68]](https://stack.convex.dev/presence-with-convex%23:~:text=Presence%2520performance).
- **Use of an established pattern:** The presence approach we choose is influenced by Convex's own example utilities[[70]](https://stack.convex.dev/presence-with-convex%23:~:text=To%2520make%2520it%2520easy%2520to,in%2520rooms%2520they%25E2%2580%2599re%2520allowed%2520in). They even provided a hook usePresence which we can mimic or use. It suggests storing partial updates and merging state per user easily[[71]](https://stack.convex.dev/presence-with-convex%23:~:text=The%2520main%2520difference%2520is%2520that,will%2520have%2520the%2520latest%2520values). That pattern ensures that, for instance, if we update a user's cursor in one part and their typing status in another, we merge into one object per user, avoiding conflicts.
- **Cursor indexing challenges recognized:** We acknowledge that accurately sharing text cursor positions is a complex topic that might be beyond MVP's full scope[[64]](https://stack.convex.dev/presence-with-convex%23:~:text=Text%2520cursors%2520are%2520an%2520important,it%2520at%2520that%2520for%2520now). We opt for a pragmatic partial implementation (maybe block-level presence or approximate indices) to still give some awareness. This decision not to fully solve it is backed by the Convex article noting it could be an entire post on its own, so we focus on simpler aspects first.
- **Ephemeral vs persistent trade-off:** Presence data is semi-ephemeral; we care about "now" and short-term history (like last seen). By storing in Convex (which persists by default), we do get durability for lastActive, which is useful for showing when someone was last online (if within a timeframe). But ephemeral parts like exact cursor moves we might not log in detail. We'll treat presence table as mostly current state that gets overwritten often, not a log of every movement. That's acceptable since slight misses aren't critical.
- **Alternative requiring separate infra unnecessary:** We could have used something like WebRTC or Y.js awareness for presence, but since we already have Convex open for collab, it's simplest to piggyback on it. Y.js has an awareness API, but adopting Y just for presence is heavy. Our solution uses what we have.
- **Scalability:** Presence data per document is relatively small (just one entry per user). Even if dozens of users, that's fine. The updates are frequent but small JSON writes. Convex's design of reactivity and caching ensures that, for example, if 50 users are online, the presence query recomputes only once per change and fan-out is handled efficiently[[24]](https://stack.convex.dev/presence-with-convex%23:~:text=Caching). This helps scale to large collab sessions.

Alternatives Considered

- **No presence indicators:** We considered deferring presence to later (to focus on core editing). But given modern expectations and the minimal effort to at least show online users, we chose to include it in MVP.
- **Polling instead of realtime:** We could have had clients poll for others' presence (e.g., query every few seconds). But that is less efficient and real-time. Convex's push obviates polling. So, we prefer push to get instant updates and lower latency.
- **Using an external pub-sub (like Pusher or Ably):** Not needed since Convex covers the basics. Those services might handle cursors at high freq slightly better if fine-tuned, but integrating another service is overkill and would duplicate what Convex already provides.
- **Full CRDT for cursors:** There are CRDT approaches to share cursors (each cursor as an item in a CRDT list). This seemed too complex to integrate just for this small feature. We left that aside.
- **Browser Peer Awareness:** Perhaps leveraging something like WebRTC broadcast for cursor positions to avoid server roundtrip. That would be complicated to orchestrate (especially with varying participant sets) and not justified at our scale. So we stick with server-mediated.
- **Only track presence, no cursors:** As an MVP of presence, we thought about just showing who's present and skip showing their cursor positions. But adding cursors (even approximately) greatly enhances the collab feeling (like Google Docs colored cursors). We decided to attempt it, and if it's not perfect, it's still useful. Worst case, we show a name label where they are editing last known location.

Implications

- **Privacy:** In some cases, presence might raise privacy concerns (someone might not want others to know they are viewing a particular document). Since this is internal collaboration, likely fine, but we might later consider a "stealth mode" or similar if needed. For now, all presence is visible to those who have access to the document.
- **Potential Noisy Updates:** If we update presence too often (like every keystroke), it could overload things. We mitigate by batching and throttling. We should fine-tune the heartbeat interval and perhaps only send cursor changes on significant moves or after idle. We might use requestAnimationFrame or setInterval on client to limit frequency.
- **Consistency with Editor State:** If a user's connection is poor, their presence heartbeats might drop, and they could appear offline while still editing. We might choose a grace period (e.g., consider them online up to 10 seconds after last heartbeat)[[62]](https://stack.convex.dev/presence-with-convex%23:~:text=A%2520common%2520way%2520to%2520detect,resources%2520your%2520app%2520will%2520consume) to avoid flicker. If they truly disconnect, after that we mark offline.
- **UI Overload:** If many users are present, showing dozens of cursors can be overwhelming. Possibly cap at showing, say, first 5 cursors and an "+n others" indicator. The data model can have all, but UI may simplify. We won't likely hit that in MVP testing, but something to note.
- **Conflict with Selection and Editing:** We need to ensure showing other's cursors does not interfere with local editing. Typically, these are rendered as decorations (e.g., in ProseMirror one can add a plugin to render remote cursors as decorations). We might integrate such a plugin that uses our presence data. If not, we do manual highlighting by overlaying colored caret elements at approximate positions in the DOM. This is an implementation challenge but known territory (ProseMirror has examples in collab module for cursors).
- **Use of Colors and Identity:** We should ensure each user gets a consistent color per document session. Possibly hash userId to a color or keep an in-memory map. If needed, store the color in presence table (the first one online picks a color and writes it, others read it). The Convex presence utility code shows an example of using emojis or so for facepile, but we'll likely do colored circles or avatars.
- **Interference with Convex Auth Session:** If Convex uses session cookies, multiple tabs of the same user might show up as separate presence entries unless we unify by userId. We might consider each tab a separate presence or merge them. Merging could cause flicker (like if one tab idle but another active). We likely treat each userId as one presence per doc, regardless of tabs. So if user opens doc in two tabs, either they appear once (with lastActive from either tab) or we differentiate by session. Simpler: one per user per doc – update on any activity from any tab. Achieved by using userId as key (the utility used a random ID per client if not logged in, but with auth we have stable).
- **Data Cleanup:** If a user closes a doc without explicitly logging out or calling an API, our heartbeat mechanism will eventually mark them offline (when heartbeats stop and lastActive ages out). We could also use the browser page unload event to inform the server (but that's unreliable and not guaranteed to run). We'll rely on timeout.
- **Integration with Permissions:** We should ensure presence doesn't leak info about restricted docs. Only users who have access to a doc will subscribe to its presence. So if you don't have doc access, you won't see presence of it, obviously. But consider if a doc is open and someone without access tries to see who is in it – they can't query that presence without auth to doc anyway. So fine. Another angle: if a doc becomes restricted (user removed) while they have it open, ideally their presence entry should be removed or hidden. If permission changes, our queries might now filter them out; or we could remove their presence entry as part of revoking access (kick them out). That might be an advanced scenario; MVP might not handle dynamic permission changes elegantly (someone could theoretically still see content until they refresh and get denied).
- **"Last seen" feature:** We have lastActive timestamps; we could show "Bob was last online 3 minutes ago" on contacts or on each document's share list. This is a nice-to-have possibly later. The data is there. We might just keep it around.
- **Cross-document presence:** We might also want to show a global status (user is online somewhere). We can derive that by looking at all presence or having a separate user status table (like user's general last seen). Convex presence example likely covers global online status too. If needed, we could extend presence system to mark user online globally (maybe update a user table on any activity). Not prioritized, but possible extension (like a "green dot" next to user name globally if any presence entry of them is active).
- **Notification tie-ins:** If we know who is present, we could avoid sending them notifications (like email or push) for changes because they already see it. That could be a future optimization using presence info.
- **Agent/AI Presence:** If we have AI agents participating (for example, an AI "co-scientist" might join a doc to make suggestions), we could also reflect that in presence ("AI Bot is online"). This could be interesting to signal when an agent is working on that content. We'd implement it by having the agent's processes update presence just like a user. The architecture supports it as long as agent has an ID.

Expansion Path

- **Precise Collaborative Cursors:** Implement a robust solution for tracking text cursors through document changes. Possibly use ProseMirror plugin that broadcasts cursor positions mapped to a specific version, and server stores it with version reference. If we adopt Y.js in future, its awareness API could manage cursors with automatic CRDT mapping. Also, if ProseMirror's collab plugin had a recommended approach, follow that (maybe by embedding a special marker nodes or using the example code from Marijn which mentions mapping positions via position maps for cursor adjustment).
- **Selections and Annotations:** Expand from single cursor (caret) to full text selections (range). We could show if someone has a chunk of text highlighted (useful in editing or when discussing text). This would require storing start and end positions in presence data. It's a natural extension once single position works.
- **Multiple Presence Scopes:** Right now presence is per document. We might generalize it to other scopes too: e.g., presence in a project or on the platform in general (like an online users list). The infrastructure can handle multiple room IDs (documentId being the room). We could implement another presence query for "global" or per space. This might be useful if we want to show a global "online now" list.
- **Activity Indications:** Possibly extend presence to show more detailed activity, e.g., "Alice is editing Title block" or "Bob is commenting". This could be an extension where presence entries include a short description of current activity. The client could update that contextually.
- **Integration with Chat or Comments:** If we add an in-app chat or commenting system, presence could tie in (like show who's typing a comment, etc.). We might either extend the presence schema or run separate presence for chat channels. But conceptually similar.
- **Scale Considerations:** If in some scenario a document is public and hundreds of people view it, our presence system might be strained (lots of entries and updates). We might need to optimize by not tracking every single viewer in real-time if count is huge (maybe just count them or sample). Or partition presence updates (like only track editors, not passive viewers, if needed). Or throttle frequency more for large N. This is a future scale problem.
- **Mobile or Low-bandwidth Mode:** In some cases, a user on a poor connection might want to disable live cursors to save bandwidth. We could have an option to turn off receiving those updates (just unsubscribe from presence query, or have server send minimal). Not a priority but worth noting as a user setting maybe.
- **Storing Historical Collaboration Data:** Though presence is ephemeral, one might want to record collaboration events (for analytics, e.g., how many simultaneous editors, peak times). We could push presence changes into a log for analysis. Or at least count minutes of active editing by user (which could be derived from heartbeats). This is beyond MVP but a potential metric (for engagement analytics or user behavior study).
- **Integration with Workflow (Mastra):** If an agent (Mastra) is assisting, perhaps it could use presence information to decide when to intervene (e.g., if user is actively typing, maybe don't distract them with suggestions). So we could expose presence data to the agent logic or to other parts of system as needed. This just means presence is a known global state accessible via queries to any function/agent.
- **UI Animations and Polish:** We can further refine how we show presence, e.g., smoothly animate cursors moving if updates are sparse (interpolate positions). Also, showing typing indicator (like "... is typing" next to name) when status=typing. Or small pop-ups when someone enters/leaves the doc ("Alice joined"). These are UX touches to consider as we refine the feature set beyond MVP core functionality.

ADR 8: Platform Boundaries and Expansion Plan (Convex, Vercel, Workflow, Mastra)

Context

The MVP system is built with a **Next.js frontend on Vercel** and a **Convex backend**. It's important to delineate what each part is responsible for, to avoid confusion and to create a clean architecture that can grow. Additionally, while MVP does not include federated multi-server setups or advanced AI agent features, the architecture should be conceived with a path to those features. This ADR covers how we structure the boundaries between front-end and backend, how we incorporate Convex's special capabilities like scheduling/workflow, and how we plan for eventual introduction of an AI agent system (Mastra) and possibly federated or multi-component architecture.

Decision

**Maintain a clear client-server separation with Next.js (Vercel) as the frontend/UI layer and Convex as the authoritative backend for data and realtime collaboration, and leverage Convex's workflow and scheduling for background tasks.** The system is initially a single Convex deployment and a single Next.js app, but we outline how to extend beyond that. Key points:

- **Next.js (Vercel) Frontend:**
- Renders the application UI, handles routing (public routes for pages, maybe protected via Next middleware to ensure login).
- Uses the Convex React client (ConvexProvider and auto-generated hooks) to call backend functions and subscribe to queries. All data mutations and fetches go through Convex APIs – we avoid maintaining any significant client-side state that isn't also in Convex, except transient UI state.
- Next.js may also do server-side rendering (SSR) for initial loads, but since most data is behind auth and dynamic, we might rely on client-side rendering after login. If SSR is needed (for public pages or SEO), we can call Convex from getServerSideProps for example to fetch some data (Convex supports Node context).
- We use Vercel's deployment for fast global delivery of static assets and edge network. But our dynamic data comes from Convex's endpoint (which by default is a single region endpoint, though Convex might have edge caching of queries).
- Security: We ensure no secret logic is in Next – it's all in Convex functions. Vercel's edge can be used for additional protections (like bot protection, etc., possibly via Vercel's built-in features if needed).
- **Convex Backend:**
- Houses the database (blocks, edges, steps, presence, etc.), and all business logic in Convex functions (queries, mutations, actions, crons).
- We treat Convex as the **source of truth** for application state. No other database is used. This centralizes consistency and simplifies sync (Convex keeps clients in sync via reactivity).
- Realtime collaboration, as described in prior ADRs, is implemented in Convex (e.g., ProseMirror OT functions, presence subscriptions). Convex's **open-source backend platform** keeps the app in sync in realtime[[72]](https://vercel.com/docs/integrations%23:~:text=Convex), which is one of the reasons we chose it.
- We take advantage of Convex **Components** (like Files Control, Prosemirror Sync, etc.) to accelerate development. These are modular backend pieces that integrate with our code.
- **Workflow & Scheduling:** Convex offers a "Workflow" system and cron jobs for background tasks. We use cron for periodic jobs (cleanup expired files, presence trim, etc., as mentioned in earlier ADRs). For more complex or multi-step background tasks, we will use Convex actions (which run without automatically re-running on data change, suitable for one-off tasks). If something long-running is needed (like an AI call that might take many seconds), we can offload that to a Convex async job or an external service but orchestrated via Convex's workflow component if possible. The mention of "Workflow component" implies Convex has durable functions that survive restarts and can handle retries. We will use this for reliability in tasks like sending email notifications or scheduling embargo lifts.
- **Auth:** Likely, we'll use Convex's built-in Auth (which can integrate with Google/GitHub OAuth, etc.). The Next frontend triggers login flows, obtains an ID token, which Convex uses to authenticate. We do not have a separate auth server. This means the client and Convex share user identity context seamlessly.
- **Mastra (AI & Agents) – Future Integration:**
- Mastra is a TypeScript AI framework (with multi-agent workflow) that Vercel is promoting[[73]](https://vercel.com/docs/ai-gateway/framework-integrations/mastra%23:~:text=Mastra%2520is%2520a%2520framework%2520for,model%2520management%2520and%2520routing%2520capabilities). In the future, we plan to incorporate AI "co-scientist" capabilities, possibly by employing Mastra to build agents that can interact with our knowledge base.
- Our plan is to keep the AI logic **modular**. For example, we might create a separate service or serverless function (could even be on Vercel using the AI SDK) that handles agent reasoning. The agent would communicate with Convex via API (either by calling Convex functions or via Convex HTTP endpoints).
- We ensure our architecture can accommodate this by: not baking in assumptions that only humans access data. We may have Convex functions that can be triggered by external calls (Convex supports HTTP endpoints or calling functions via API). So an agent could call convex.actions.queryKnowledgeGraph to get info, then decide something, then call a mutation to add a block or edge with its findings.
- If Mastra is integrated in-process: Possibly, we could run a Mastra agent within a Convex action (since Mastra is just TypeScript, and Convex actions can call external APIs). However, Mastra might have heavier requirements or long-lived processes. Another approach: use Vercel Edge Functions or AWS Lambda to run agents asynchronously, and have them call back to Convex. The specifics will depend on performance. For now, we note that "Mastra is a framework for building AI-powered features on a modern JS stack"[[73]](https://vercel.com/docs/ai-gateway/framework-integrations/mastra%23:~:text=Mastra%2520is%2520a%2520framework%2520for,model%2520management%2520and%2520routing%2520capabilities), which suggests it fits well with our Next/Convex stack. We will likely orchestrate via the Vercel AI SDK and ensure our platform boundaries allow it (perhaps treat it as another client or microservice).
- **No Federated Architecture at MVP, but future expansion:**
- MVP is one Convex deployment (one database, one backend). In future, if we needed multiple backends (for either multi-tenancy or scaling or federation between communities), we'd plan to connect them via an API layer or message passing. For example, one could imagine a scenario where some blocks are hosted in other Convex deployments and referenced remotely. Not in MVP, but our block model (ADR 1) could be extended with a field for origin or global ID to enable references to external content.
- If needed, we might develop a **federation server** that sits above multiple Convex backends to route requests based on content location or user domain. Alternatively, adopt an open standard like ActivityPub to let different instances follow each other's public content. That would be an extension where our backend exports certain events and consumes from others. The decision is to keep MVP simple (no federation), but we will keep data model flexible enough (like globally unique IDs not assuming a single instance, maybe use UUIDs or something not tied to an auto-increment).
- **DevOps and Deployment:**
- Vercel and Convex are both hosted services. Deployment is separate: we deploy Next.js to Vercel (which gives us a domain) and deploy Convex functions to Convex cloud. They communicate over the internet. We will secure that by using SSL and by Convex's auth requirement. Also, because Convex functions are like serverless, we trust Convex's scaling.
- Logging/Monitoring: We rely on Convex's dashboard for backend logs, and Vercel for any front-end logs. If needed we might integrate Sentry or similar for the frontend. Convex likely has some solution for error tracking as well.
- **Edge concerns:** Vercel runs CDN nodes globally for static assets. Convex by default might run in one region (we can pick e.g. US or wherever majority users are). For global users, that might introduce latency on data calls. However, the volume is not high per call, and websockets maintain persistent connections. In future, if needed, Convex might allow multi-region or we could deploy separate Convex instances per region and connect them (non-trivial). We accept some latency tradeoff for simplicity. Perhaps ensure to choose a region that's acceptable (maybe US-east which is a good compromise).
- **Workflow (Convex) usage examples in MVP:**
- Cron jobs as described (cleanup tasks).
- Possibly using Convex's durable functions if we have processes like "after a user edits a block, run an AI summarizer in background". We could schedule that via a Convex action with retries.
- The mention of "Workflow component" in Convex docs suggests an orchestrator for multi-step tasks[[74]](https://stack.convex.dev/presence-with-convex%23:~:text=AI%2520Agents%2520with%2520Built). For example, if we integrate an agent that needs to do a sequence: search knowledge, analyze, write result, we could chain those in a durable function to survive restarts.
- We'll incorporate that approach for any AI or heavy tasks as needed in expansion. For MVP, perhaps not heavily used except scheduled jobs and maybe file cleanup.

Rationale

- **Separation of Concerns:** By clearly delineating front-end and backend responsibilities, we make the system easier to maintain. Next.js deals with presentation and user interaction, Convex deals with data consistency and business rules. This way, one can change UI without risking backend logic and vice versa.
- **Leverage Specialized Platforms:** Vercel is optimized for frontend delivery (CDN caching, asset optimization, easy deployment), and Convex is optimized for backend state with real-time sync. Using each for what it's best at avoids reinventing wheels. For instance, we didn't attempt to make Next.js do real-time synchronization or host a database – we offload that to Convex which provides a cohesive solution.
- **Scalability and "scale as you grow":** This architecture can start small but grow without major changes. For example, if usage spikes, Convex can scale horizontally behind the scenes, and Vercel will scale our frontend globally (they auto-scale serverless functions and static content). If we need to add more features (like search, AI, etc.), we can often integrate via Convex components or Vercel integrations without overhauling the whole stack.
- **Expansion Readiness:** We consciously consider expansions:
- **Agents/AIs:** We know that to integrate AI workflows (Mastra), we may need to interface with new services. By keeping our core logic in Convex, we can either call out to AI APIs from Convex or have the AI call into Convex. We avoid putting AI logic in the client because that could expose secrets or be inconsistent. Instead, running it server-side (either in Convex or an allied serverless function) is more secure and reliable. Mastra being a Vercel-integrated framework[[75]](https://vercel.com/docs/ai-sdk%23:~:text=AI%2520SDK%2520,servers%2520%25C2%25B7%2520Vercel%2520MCP) means it likely runs on the server side (or edge side) as well, coordinating with our data.
- **Federation:** While not needed now, our design of using globally unique block IDs and not assuming a single monolithic server means we could, in future, assign blocks URNs like block:<instance>:<id> to reference remote ones. We haven't explicitly done that yet, but we aren't using things like auto-increment IDs that would clash. Convex uses its own IDs (likely UUIDs or similar) so those could incorporate instance if needed. Also, since all access is via our functions, introducing a federation layer can be done at function level (for example, getBlock might later detect if block not in local DB and then fetch from another server).
- **Monolithic vs Microservices:** We opt for a **monolithic backend** (Convex functions cover everything) rather than splitting microservices for MVP. This reduces complexity early on. However, we acknowledge that adding something like a vector search or heavy ML might be outside Convex's current feature set. In such cases, we can add a microservice just for that piece:
- For example, if we want semantic search, perhaps we use an external vector DB (Pinecone, etc.). We then have a Convex function call that service. Or we integrate Convex's upcoming vector search if available (Convex has mention of vector search in marketing[[76]](https://stack.convex.dev/presence-with-convex%23:~:text=Build%2520in%2520minutes,%2520scale%2520forever)).
- The plan is to extend within Convex's ecosystem as far as possible (since they are adding components e.g. an AI memory component[[77]](https://stack.convex.dev/presence-with-convex%23:~:text=Deployment), vector search, etc.). Only if necessary, spin out separate services.
- **Security Boundary:** By having all sensitive operations on the Convex side, and having Next.js essentially serve static and call APIs, we reduce the risk of exposing secrets. The Next.js environment will hold nothing more than perhaps the Convex deployment URL and a public auth configuration. All secret keys (if any, e.g. for third-party APIs or Mastra integration) will reside in Convex or Vercel serverless function environment variables, not in client code.
- **Consistency:** Running everything through one backend (Convex) ensures consistency in data. It's effectively like having a single database with all constraints and logic in one place. If we had multiple backends (say one for collab, one for files, one for AI), we'd have to synchronize them, which is complex. Using Convex components for various needs (files, etc.) keeps it within one transactional system where possible.
- **Utilizing Vercel/Convex integration:** Vercel has an integration marketplace and Convex is one of the storage/backends listed[[72]](https://vercel.com/docs/integrations%23:~:text=Convex). This indicates that Vercel and Convex work smoothly together (e.g., environment config, easy deployments). By sticking to these integrated solutions, we benefit from community and official support.

Alternatives Considered

- **Single Server (no Convex):** Use a Node.js server (or Next.js API routes) with a database like Postgres or Mongo, and maybe something like ShareDB or Yjs for collab. We did not choose this because it would require building a lot from scratch (real-time sync, OT logic, etc.), and we'd lose Convex's elegant reactivity. Also, scaling that on Vercel might be tricky (WebSockets on Vercel are not straightforward, one might need a separate WebSocket server).
- **Fully Serverless (no dedicated backend):** There was an option to try to do everything with client and maybe some edge functions or a combination of services (e.g., using Firebase for database and auth, maybe a CRDT library for collab). This patchwork would be harder to maintain and might not achieve the tight integration we want (for example, merging CRDT states and ensuring security would be challenging).
- **Integrating AI at MVP baseline:** One thought: include an AI agent from day one. We decided against it to keep MVP focused and not over-complicate initial architecture. We want the architecture to allow adding it, but not necessarily have it now. If we had tried to include it, we might have needed more components (like a vector DB or an external LLM service integration) upfront, which we can postpone.
- **Ignoring expansion concerns:** We could have built MVP ignoring federation or multi-agent future, which might lead to shortcuts that later hinder expansion. We consciously avoided such shortcuts. For example, we use globally unique IDs (not sequential IDs tied to single DB) to ease future merging of data. We structure code so that adding new Convex functions or even new services won't require tearing apart the monolith (e.g., we could factor out AI-related functions to a separate module or even separate deploy if needed).
- **Monorepo vs separate repos:** We considered whether to keep frontend and backend in one repository. Possibly, yes – Convex functions can live in the same codebase as the Next app for convenience (Convex has a dev server that watches file changes). This synergy can be beneficial (shared types between front and back, etc.). On the other hand, logically they are decoupled by API. We think a monorepo is fine (one Vercel project with Convex directory inside, or two projects with shared code). The decision doesn't heavily impact runtime but is about dev experience. We lean monorepo to share code (like types for data models).
- **Third-party integrations early vs late:** There are many Vercel integrations (for logging, auth, etc.). We stick with minimal needed: Convex for backend, maybe Vercel's own AI SDK when time comes. We decided not to integrate something like Sentry from start, but that's easily added if needed for error monitoring. Similarly, no separate analytics yet (maybe just use Vercel/Next analytics or simple logging in Convex).

Implications

- **Vendor Lock-in:** Choosing Convex and Vercel ties us somewhat to these platforms. If in future we needed to self-host or move to open source stack, we'd have some migration work (Convex has an open source core but not sure if fully self-hostable easily; Vercel is just hosting, we could move to our own Next.js server if needed). We accepted this lock-in trade-off for faster development. We keep our code structured (Convex logic is mostly in functions that could potentially be ported to another server framework if absolutely necessary).
- **Costs:** Both Vercel and Convex have cost models (free tiers and usage-based pricing beyond). We should monitor usage (e.g., Convex function calls, data storage) to avoid surprises. If AI is added, that likely brings API costs too. Having clear boundaries helps here: e.g., we could swap out Convex for an in-house solution if cost demands (not trivial, but contained).
- **Single Point of Failure:** One Convex deployment is one point of failure. If it goes down, the whole app's dynamic features fail. Vercel and Convex both have high availability but not guaranteed 100%. Mitigation in future might be to have a read-only cache or static export of some content for public view if Convex is down. But MVP likely doesn't require that level of redundancy.
- **Development Workflow:** Developers will need to run both Next and Convex dev servers. Fortunately, Convex provides a dev server that runs in background and we can use environment variables to connect Next to that. The integration is decent (just be aware of syncing types).
- **Edge Functions on Vercel:** If we need to run some logic closer to users (like middleware, custom auth redirect, or soon maybe some AI model at edge), we have option to use Vercel Edge Functions. Right now, not heavily used. Possibly for optimizing static site generation or caching public pages, we could do something at edge. But as most content is dynamic and behind login, edge caching is less relevant except maybe caching public read-only content post-embargo.
- **Stateful vs Stateless boundaries:** Convex is stateful (holds DB state). Next/Vercel is stateless between requests (except what's in browser memory). That split means we must be careful to not attempt to store state in Next server memory (which could be ephemeral and multi-instance). All persistent state must go to Convex (or client local state if ephemeral on that client). Also, any server-side rendering should fetch fresh state from Convex rather than assume previous knowledge.
- **Feature development:** When adding new features, we'll decide if it's primarily frontend (just UI changes) or requires backend changes (new Convex functions, tables). We foresee many features involve both. The good thing is our unified language (TypeScript) for both, and likely some shared types (Convex can share types to client via codegen). That accelerates feature dev and reduces mismatch errors.
- **Mastra introduction:** When we actually bring in Mastra, we will allocate possibly a separate set of Convex functions to interface with it. Or maybe run Mastra within an action (depending on how heavy it is). Since Mastra is basically an AI agent framework, if it needs to maintain agent state or run a loop, we might use Convex's persistent storage to store agent memory (they even have an example of "AI Agents with Built-in Memory" on Convex stack[[74]](https://stack.convex.dev/presence-with-convex%23:~:text=AI%2520Agents%2520with%2520Built)). That suggests synergy: agent's memory (like conversation logs or knowledge) stored in Convex, agent logic runs in either Convex actions or in Vercel functions using the Vercel AI SDK and calls Convex to fetch memory or update results.
- **Workflow Orchestration Example:** Suppose we have a feature "Summarize all evidence for this claim". Implementation might be: Convex action fetches relevant blocks (maybe heavy query across edges), sends the text to an LLM API (like OpenAI) for summarization, gets result, inserts a new block with summary. This sequence can fail at API call or take time. Using Convex's workflow (durable function with retry) ensures it will complete even if there's a transient failure. This is how we plan to use workflow, ensuring reliability for such tasks. That's beyond core MVP, but may appear as soon as we integrate AI.
- **APIs for external integration:** If we or others want to integrate with this knowledge system from outside (say, an external script or service needs to query data or add a block), how to do it? Convex provides an HTTP API (Convex HTTP endpoints or using API keys to call functions). We likely will expose some endpoints for integration. This foresight means we'll design Convex functions with clean interfaces that could later be exposed externally (with appropriate auth, maybe using API keys or service accounts). For example, in a federated scenario or a plugin ecosystem, we might let external tools call createBlock or queryEdges.
- **Logging and Monitoring in Production:** As we expand, monitoring performance and errors becomes crucial. Convex provides some telemetry, and Vercel provides logs for serverless function usage. We may add instrumentation (e.g., measure how long collab merges take, etc.). If something becomes a bottleneck (like a particular query slow as data grows), we'll consider caching or indexing more. At least in design, we keep heavy loops out of query functions (Convex queries should ideally just fetch by indexes, not do crazy compute).
- **Continuous Deployment:** We intend to deploy often. Vercel and Convex both support CI/CD style deployment. They may not automatically coordinate (i.e., deploying new Convex schema and new frontend simultaneously), so we'll manage that (maybe deploy Convex first, ensure functions are updated, then deploy frontend that calls them). In local dev, we test changes to both. This is manageable but something to watch when doing migrations (like changing Convex schema might need careful roll-out).
- **Documentation for Devs:** We'll document these boundaries for any new developers: e.g., "All data mutations go in Convex functions. Do not attempt to mutate application state in Next.js side. Next.js should only call Convex or render UI." Also, "If adding a new background job, consider using Convex cron or action rather than building a separate scheduler."

[[1]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,interrelate) [[2]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,offs) [[27]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=,specific%2520information) [[28]](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b%23:~:text=We%2520have%2520directed%2520graphs%2520where,the%2520shortest%2520routes%2520between%2520nodes) Understanding Knowledge graphs for Agentic AI | by Shilpa Thota

[https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b](https://shilpathota.medium.com/understanding-knowledge-graphs-for-agentic-ai-7162d018387b)

[[3]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=changes%2520in%2520a%2520different%2520order,will%2520produce%2520the%2520same%2520document) [[8]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Like%2520OT,%2520ProseMirror%2520uses%2520a,will%2520produce%2520the%2520same%2520document) [[12]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=The%2520design%2520decisions%2520that%2520make,be%2520interesting%2520to%2520work%2520on) [[13]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=changes%2520on%2520top%2520of%2520them,,before%2520retrying%2520the%2520push) [[16]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=By%2520using%2520a%2520central%2520server,,them,%2520before%2520retrying%2520the%2520push) [[17]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=Position%2520Mapping) [[19]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=1,front%2520of%2520the%2520change's%2520offset) [[20]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=But%2520you%2520can%2520save%2520oh,their%2520Google%2520Docs%25E2%2580%2594a%2520centralized%2520system) [[26]](https://marijnhaverbeke.nl/blog/collaborative-editing.html%23:~:text=And%2520I%2520don't%2520actually%2520believe,have%2520not%2520actually%2520tried%2520this) Collaborative Editing in ProseMirror

[https://marijnhaverbeke.nl/blog/collaborative-editing.html](https://marijnhaverbeke.nl/blog/collaborative-editing.html)

[[4]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,side,%2520enabling%2520easy%2520AI%2520interoperation) [[5]](https://github.com/get-convex/prosemirror-sync%23:~:text=const%2520prosemirrorSync%2520=%2520new%2520ProsemirrorSync\(components,//) [[6]](https://github.com/get-convex/prosemirror-sync%23:~:text=Configuring%2520the%2520snapshot%2520debounce%2520interval) [[7]](https://github.com/get-convex/prosemirror-sync%23:~:text=*%2520Server,for%2520old%2520snapshots%2520&%2520steps) [[10]](https://github.com/get-convex/prosemirror-sync%23:~:text=,side,%2520enabling%2520easy%2520AI%2520interoperation) [[11]](https://github.com/get-convex/prosemirror-sync%23:~:text=Configuring%2520the%2520snapshot%2520debounce%2520interval) [[14]](https://github.com/get-convex/prosemirror-sync%23:~:text=,and%2520doesn't%2520have%2520local%2520changes) [[25]](https://github.com/get-convex/prosemirror-sync%23:~:text=There%2520can%2520be%2520races,%2520but,and%2520are%2520safe%2520to%2520apply) GitHub - get-convex/prosemirror-sync: Sync prosemirror documents with Convex for server-authorized collaborative editing

[https://github.com/get-convex/prosemirror-sync](https://github.com/get-convex/prosemirror-sync)

[[9]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=The%2520gist%2520is%2520that%2520we,them%2520back%2520to%2520the%2520clients) [[15]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=During%2520sessions%2520with%2520a%2520high,and%2520the%2520client%2520backlogs%2520clear) [[18]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=ProseMirror,at%2520which%2520point%2520they%2520must) [[21]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=There%2520are%2520a%2520number%2520of,a%2520lot%2520of%2520extra%2520complexity) [[22]](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/%23:~:text=Can%2520we%2520eliminate%2520the%2520need,work%2520closer%2520to%2520the%2520database) ProseMirror Collab Performance | Blog

[https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/](https://stepwisehq.com/blog/2023-07-25-prosemirror-collab-performance/)

[[23]](https://stack.convex.dev/presence-with-convex%23:~:text=Convex%2520helped%2520make%2520this%2520much,WebSocket%2520reactivity%2520and%2520caching%2520scalability) [[24]](https://stack.convex.dev/presence-with-convex%23:~:text=Caching) [[61]](https://stack.convex.dev/presence-with-convex%23:~:text=Presence,%2520as%2520we%25E2%2580%2599ll%2520use%2520the,in%2520a%2520Google%2520Doc,%2520etc) [[62]](https://stack.convex.dev/presence-with-convex%23:~:text=A%2520common%2520way%2520to%2520detect,resources%2520your%2520app%2520will%2520consume) [[63]](https://stack.convex.dev/presence-with-convex%23:~:text=Reactivity) [[64]](https://stack.convex.dev/presence-with-convex%23:~:text=Text%2520cursors%2520are%2520an%2520important,it%2520at%2520that%2520for%2520now) [[65]](https://stack.convex.dev/presence-with-convex%23:~:text=For%2520mouse%2520cursors,%2520the%2520challenge,where%2520the%2520cursor%2520is%2520moving) [[66]](https://stack.convex.dev/presence-with-convex%23:~:text=Sharing%2520mouse%2520cursor%2520positions%2520are,the%2520data%2520is%2520especially%2520ephemeral) [[67]](https://stack.convex.dev/presence-with-convex%23:~:text=A%2520%25E2%2580%259Cfacepile%25E2%2580%259D%2520is%2520a%2520popular,in%2520mind%2520when%2520building%2520them) [[68]](https://stack.convex.dev/presence-with-convex%23:~:text=Presence%2520performance) [[69]](https://stack.convex.dev/presence-with-convex%23:~:text=someone%2520is%2520composing%2520a%2520message,in%2520a%2520Google%2520Doc,%2520etc) [[70]](https://stack.convex.dev/presence-with-convex%23:~:text=To%2520make%2520it%2520easy%2520to,in%2520rooms%2520they%25E2%2580%2599re%2520allowed%2520in) [[71]](https://stack.convex.dev/presence-with-convex%23:~:text=The%2520main%2520difference%2520is%2520that,will%2520have%2520the%2520latest%2520values) [[74]](https://stack.convex.dev/presence-with-convex%23:~:text=AI%2520Agents%2520with%2520Built) [[76]](https://stack.convex.dev/presence-with-convex%23:~:text=Build%2520in%2520minutes,%2520scale%2520forever) [[77]](https://stack.convex.dev/presence-with-convex%23:~:text=Deployment) Implementing Presence with Convex

[https://stack.convex.dev/presence-with-convex](https://stack.convex.dev/presence-with-convex)

[[29]](https://www.convex.dev/components/files-control%23:~:text=Convex%2520Files%2520Control) [[30]](https://www.convex.dev/components/files-control%23:~:text=*%2520Two,for%2520presigned%2520or%2520HTTP%2520uploads) [[31]](https://www.convex.dev/components/files-control%23:~:text=//%2520Client,) [[32]](https://www.convex.dev/components/files-control%23:~:text=const%2520,json) [[33]](https://www.convex.dev/components/files-control%23:~:text=const%2520result%2520=%2520await%2520finalizeUpload\(,1000,) [[34]](https://www.convex.dev/components/files-control%23:~:text=const%2520,) [[35]](https://www.convex.dev/components/files-control%23:~:text=const%2520result%2520=%2520await%2520ctx,) [[36]](https://www.convex.dev/components/files-control%23:~:text=A%2520Convex%2520component%2520for%2520secure,plus%2520a%2520React%2520upload%2520hook) [[37]](https://www.convex.dev/components/files-control%23:~:text=//%2520convex/http.ts%2520import%2520,generated/api) [[38]](https://www.convex.dev/components/files-control%23:~:text=checkUploadRequest:%2520async%2520\(ctx\)%2520=,Type%2522:%2520%2522application/json%2522%2520%257D,%2520%257D\);) [[39]](https://www.convex.dev/components/files-control%23:~:text=//%2520Optional:%2520provide%2520accessKey%2520for,) [[40]](https://www.convex.dev/components/files-control%23:~:text=import%2520,control) [[41]](https://www.convex.dev/components/files-control%23:~:text=) [[42]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520normalized%2520,empty%2520value) [[43]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520normalized%2520,empty%2520value) [[44]](https://www.convex.dev/components/files-control%23:~:text=Cleanup) [[45]](https://www.convex.dev/components/files-control%23:~:text=Use%2520,it%2520in%2520a%2520cron%2520job) [[46]](https://www.convex.dev/components/files-control%23:~:text=) [[47]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520not%2520placed,consumeDownloadGrantForUrl) [[48]](https://www.convex.dev/components/files-control%23:~:text=Set%2520,to%2520return%2520an%2520access%2520key) [[49]](https://www.convex.dev/components/files-control%23:~:text=const%2520grant%2520=%2520await%2520ctx,passphrase%2522%2520%257D,) [[50]](https://www.convex.dev/components/files-control%23:~:text=React%2520hook) [[51]](https://www.convex.dev/components/files-control%23:~:text=//%2520Presigned%2520await%2520uploadFile\(,) [[52]](https://www.convex.dev/components/files-control%23:~:text=Access%2520control%2520&%2520queries) [[53]](https://www.convex.dev/components/files-control%23:~:text=handler:%2520async%2520\(ctx,%2520args\)%2520=,Unauthorized) [[54]](https://www.convex.dev/components/files-control%23:~:text=,Unauthorized) [[55]](https://www.convex.dev/components/files-control%23:~:text=) [[56]](https://www.convex.dev/components/files-control%23:~:text=Access%2520keys%2520are%2520normalized%2520,empty%2520value) [[57]](https://www.convex.dev/components/files-control%23:~:text=Shareable%2520links) [[59]](https://www.convex.dev/components/files-control%23:~:text=import%2520,generated/api) [[60]](https://www.convex.dev/components/files-control%23:~:text=storageId,%2520maxUses:%25201,%2520expiresAt:%2520Date,) Files Control

[https://www.convex.dev/components/files-control](https://www.convex.dev/components/files-control)

[[58]](https://jumpcloud.com/it-index/what-is-time-based-access-control%23:~:text=Time,based%2520on%2520predefined%2520time%2520constraints) What is Time-Based Access Control? - JumpCloud

[https://jumpcloud.com/it-index/what-is-time-based-access-control](https://jumpcloud.com/it-index/what-is-time-based-access-control)

[[72]](https://vercel.com/docs/integrations%23:~:text=Convex) Vercel Integrations

[https://vercel.com/docs/integrations](https://vercel.com/docs/integrations)

[[73]](https://vercel.com/docs/ai-gateway/framework-integrations/mastra%23:~:text=Mastra%2520is%2520a%2520framework%2520for,model%2520management%2520and%2520routing%2520capabilities) Mastra

[https://vercel.com/docs/ai-gateway/framework-integrations/mastra](https://vercel.com/docs/ai-gateway/framework-integrations/mastra)

[[75]](https://vercel.com/docs/ai-sdk%23:~:text=AI%2520SDK%2520,servers%2520%25C2%25B7%2520Vercel%2520MCP) AI SDK - Vercel

[https://vercel.com/docs/ai-sdk](https://vercel.com/docs/ai-sdk)
