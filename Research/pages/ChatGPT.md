---
lang: 'en'
slug: '/85E9E7'
---

## [Introducing ChatGPT Plus](https://openai.com/blog/chatgpt-plus/)

- General access to ChatGPT, even during peak times
- Faster response times
- Priority access to new features and improvements

## [Microsoft Teams Premium: Cut costs and add AI-powered productivity](https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/)

- I'm excited to share that [Microsoft Teams Premium](https://www.microsoft.com/microsoft-teams/premium) is generally available. Built on the familiar, all-in-one collaborative experience of Microsoft Teams, Teams Premium brings the latest technologies, including Large Language Models powered by [[OpenAI]]'s GPT-3.5, to make meetings more intelligent, personalized, and protected—whether it's one-on-one, large meetings, virtual appointments, or webinars

## [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)

- We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup
- We trained an initial model using supervised fine-tuning: human [[AI]] trainers provided conversations in which they played both sides—the user and an [[AI]] assistant. We gave the trainers access to model-written suggestions to help them compose their responses
- To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that [[AI]] trainers had with the chatbot
- We randomly selected a model-written message, sampled several [[alternative]] completions, and had [[AI]] trainers rank them. Using these reward models, we can fine-tune the model using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/). We performed several iterations of this process
- ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022. You can learn more about the 3.5 series [here](https://beta.openai.com/docs/model-index-for-researchers). ChatGPT and GPT 3.5 were trained on an Azure [[AI]] supercomputing infrastructure

## Examples

> Turned out to be a chat [[UI and UX|UI]] that got people to start tinkering with text.
>
> — Nat Friedman (@natfriedman) [December 4, 2022](https://twitter.com/natfriedman/status/1599206152025231360?ref_src=twsrc%5Etfw)

> I will be forwarding all future media requests to ChatGPT [pic.twitter.com/ygeBfsB07c](https://t.co/ygeBfsB07c)
>
> — Dylan Field (@zoink) [December 2, 2022](https://twitter.com/zoink/status/1598827692803051521?ref_src=twsrc%5Etfw)

> Introducing SiriGPT
>
> A [[Generative Pre-trained Transformer|GPT]] voice assistant built entirely with [[Shortcuts]]
>
> Tap in for setup [https://t.co/orh6Wj1XMd](https://t.co/orh6Wj1XMd) [pic.twitter.com/hlkwKoAOmq](https://t.co/hlkwKoAOmq)
>
> — Joe Kennedy (@joekndy) [December 3, 2022](https://twitter.com/joekndy/status/1598874918422450176?ref_src=twsrc%5Etfw)

### [Using ChatGPT As a Co-Founder](https://www.atomic14.com/2022/12/05/using-chatgpt-as-a-co-founder.html)

- It's pretty impressive stuff, some of it is a bit overly verbose and a bit generic, but I could probably drill into the answers and get more detail. It would probably generate most of the code for the [[Back-end|backend]] if I wanted it to:
