---
lang: 'en'
slug: '/70625E'
---


TODO: USE FUMADOCS I18N

<div lang='en-US'>

Catastrophic forgetting, also known as catastrophic interference, is a phenomenon that occurs in machine learning when a model forgets previously learned information as it learns new information. It can also be described as a failure of stability, in which new experience overwrites previous experience.

Catastrophic forgetting can occur when:

- A neural network or machine learning model "forgets" or dramatically reduces its performance on previously learned tasks after learning a new task
- An artificial neural network abruptly and drastically forgets previously learned information upon learning new information
- A network decides to work with data that's too far removed from its basic training
- A model concentrates only on task B and takes steps in the direction of its gradient
- The ability to discriminate between data from different tasks worsens

Current approaches that deal with forgetting ignore the problem of catastrophic remembering.

</div>


<div lang='ko-KR'>

치명적 간섭이라고도 하는 치명적 망각은 머신 러닝에서 모델이 새로운 정보를 학습할 때 이전에 학습한 정보를 잊어버리는 현상이다. 새로운 경험이 이전 경험을 덮어쓰는 안정성 실패라고도 할 수 있다.

치명적인 망각은 다음과 같은 경우에 발생할 수 있다:

- 신경망 또는 머신 러닝 모델이 새로운 작업을 학습한 후 이전에 학습한 작업에 대한 성능을 '잊어버리거나' 급격히 감소하는 경우
- 인공 신경망이 새로운 정보를 학습할 때 이전에 학습한 정보를 갑작스럽고 급격하게 잊어버리는 경우
- 네트워크가 기본 학습에서 너무 멀리 떨어진 데이터로 작업하기로 결정하는 경우
- 모델이 작업 B에만 집중하고 그 기울기 방향으로 단계를 밟는 경우
- 다른 작업의 데이터를 구별하는 능력이 저하되는 경우

망각에 대처하는 현재의 접근 방식은 치명적 기억 문제를 무시한다.

</div>

