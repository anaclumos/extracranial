---
lang: 'en'
slug: '/A796F1'
aliases: ['information density']
---

> In information theory, the entropy of a random variable is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible outcomes. Given a discrete random variable $X$, which takes values in the alphabet $\mathcal{X}$ and is distributed according to $p: \mathcal{X}\to[0, 1]$:
>
> $E(X) := -\sum_{x \in \mathcal{X}} p(x) \log p(x) = \mathbb{E}[-\log p(X)] ,$
>
> [Entropy (information theory)](<https://en.wikipedia.org/wiki/Entropy_(information_theory)>)

<head>
  <html lang="en-US"/>
</head>
