---
lang: 'en'
slug: '/786F87'
aliases: ["Let's Alter"]
---

## Ongoing Subprojects

- [ ] Perceptual Hashing [On Device](./../.././docs/pages/On-Device.md). [phash.wasm](./../.././docs/pages/phash.wasm.md)

## [TODO](./../.././docs/pages/TODO.md)

- [ ] PlanetScale Federated Server to store `hash → text`
- [ ] Inference Server to `image → text`. [AWS](./../.././docs/pages/AWS.md) SageMaker? Or [Hugging Face](./../.././docs/pages/Hugging%20Face.md) as an [API](./../.././docs/pages/API.md) server?
- [ ] Inference Server stores that data to Federated Server
- [ ] imgbot app

### Resources

- [Letter to Mr. Matt Rickard on 2022-11-28](./../.././docs/pages/Letter%20to%20Mr.%20Matt%20Rickard%20on%202022-11-28.md)
- [Comparing BLIP and CLIP](./../.././docs/pages/Comparing%20BLIP%20and%20CLIP.md)
- [Example References for Alter](./../.././docs/pages/Example%20References%20for%20Alter.md)
- [salesforce/BLIP](https://github.com/salesforce/BLIP): [PyTorch](./../.././docs/pages/PyTorch.md) code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- [openai/CLIP: Contrastive Language-Image Pretraining](https://github.com/openai/CLIP)
- [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- Vision Transforms (ViT)
- [CLIP Interrogator - a Hugging Face Space by pharma](https://huggingface.co/spaces/pharma/CLIP-Interrogator)
