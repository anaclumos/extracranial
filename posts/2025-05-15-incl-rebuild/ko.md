---
title: '의료AI를 위한 차세대 MLOps 프론트엔드'
date: 2025-05-15
authors: anaclumos
slug: '/493D6F'
---

<!-- truncate -->

글을 쓰지 못한 오랜 시간, 가장 많은 열정과 노력을 쏟은 것은, 의료 AI 기업 루닛에서의 MLOps 업무이다. 나는 이곳이 첫 풀타임 직장인데, FE 전체를 리드하여 개발한 경험을 공유한다. 결과적으로는 결과적으로 지난한 시간을 거쳐 성공 반, 실패 반으로 끝났는데, 그 이야기를 조심스럽게 해보려 한다.

의료 AI 기업 루닛에서는 매일 수십 종류의 ML 실험들이 일어난다. 하지만 대부분의 ML 실험 자체는 **수공업**에 가깝다. 데이터셋을 준비하고 여러 파라미터를 조정해서 실험을 돌리고 결과를 저장해 비교 분석하는 것은 AI 과학자가 직접 붙어서 번거롭게 케어를 해줘야 한다. 이 때문에 2021년 경부터 루닛에서는 INCL이라는 MLOps 플랫폼을 구축했다. 실험 코드와 추적할 지표 이름을 제공하기만 하면 자동으로 클라우드 환경에서 실험을 진행한 뒤 보기 좋은 그래프로 정리해주는 기능이다. 그러나 회사 안에서만 쓰는 앱이라는 특성 상 어쩔 수 없이 기술 부채가 심해져갔다. 그저 "동작만" 하면 되는 상태였기 때문이다. 때문에 최적화 또는 코드 정리는 항상 뒷전일 수 밖에 없었고, 결과적으로 굉장히 크고 무거운 웹 서비스가 되었다.

루닛에 합류한 작년 5월부터 나는 이것을 개선하자는 의견을 줄곧 피력해왔다. 첫째는 모든 동작이 수 초간의 로딩이 걸리는 것이 쌓이고 쌓여 연구자들의 시간을 달마다 수십 시간씩 뺏고 있다는 계산이고, 두 번째는 이 서비스를 개편하면 오픈 서비스로 만들 수 있겠다는 기대 때문이었다. 팀에서도 오픈 서비스로 나아가는 좋은 방향성이라는 것에 대해서 동의했다. 팀 입장에서는, 더 나아가 새로 추가하고 싶었던 기능들이 다양하게 있었는데 기존의 거대해진 코드베이스 때문에 변경이 매우 어려워졌다는 점, 그래서 사용자들이 요청하는 기능들이 항상 계류되고 있다는 점을 타파하기 위한 좋은 기회로 보았다. 동상이몽이었지만, 어찌 되었든 INCL을 가장 매력적인 MLOps 서비스로 만들자는 점에는 동의하고 있었다.

조그마한 요청들이 계속 들어오는데 차트나 테이블 같은 것들이 진짜 어질어질했다. 오히려 dx 개선이 기대됐다

## 하멜 표류기, 헤이그 특사, 거스 히딩크, INCL Let's go

다른 것에 앞서, 왜 이렇게 속도가 느릴 수 밖에 없는지 알아보자. 우선 첫째는 모든 클라우드 리소스가 네덜란드에 있기 때문이다. 꽤나 비직관적일텐데 (나도 그랬다) 가장 큰 이유는 2021년 당시만 해도 GPU 리소스가 네덜란드 서버에 가장 많았기 때문이다. 백엔드 서버 또한 컨테이너에서 돌아가는 수많은 데이터를 기록하고 추적해야하고, 외부에 있다면 외부통신 비용이 별도로 발생하기에, 네덜란드에 있을 수 밖에 없었고, 결과적으로 모든 자원이 네덜란드에 발이 묶여버리게 되었다. 사실상 지구 반대편에 있으니, 물리적으로 시간이 오래 걸릴 수 밖에 없었다.

하지만 예로부터 독한 병엔 극약처방이 필요하다고 했었던가?

## RSC와 SWR

이번에는 `pnpm init` 부터 다시 했던 관계로, 정말 모든 것을 변경할 수 있는 기회였다. 새로운 Next.js와 리액트 기술인 App Router와 React Server Component, 그리고 Partial Prerendering에 맞추어 초장부터 다시 시작할 수 있는 굉장히 좋은 기회였고 우리의 이 네덜란드 이슈를 단박에 해결할 수도 있는 기술적 돌파구일 수 있었다.

때문에 처음에 초기 탐색을 하며 어떻게 RSC와 SWR, 그리고 PPR을 쓸 수 있을지 각양각색으로 연구했다. 결과만 공유하자면,

- Client fetching만 사용하는 것은 속도가 너무 느려서 도움이 되지 않음. 기존의 코드를 정리하면서 조금의 속도 향상을 도모할 수 있겠으나, 큰 차이는 아닐 것임. 클라이언트 단의 단순 `fetch` 보다 빨라져야만 함.
- 실험 데이터가 실시간으로 업데이트되고 있기에, 어떤 형태로는 자동 업데이트되는 코드를 쓰고 싶다. SWR이나 React Query 등을 사용해서 정기적으로 데이터를 가져오는 형식으로 보여주면 좋을 것
- Vercel에서 권장하는 RSC, PPR은 DB단의 속도보다 빠르게 스켈레톤을 보여주기 위한 것으로, 네덜란드 왕복에 오래 걸리는 현안에 있어 큰 도움이 되지 않음. 즉, 어떤 형태로든 데이터를 서울 근처에 캐싱해둬야 함.
- 마찬가지로, Vercel에서 권장하는대로 next cache directive를 이용해 RSC를 사용하면, Stale-while-revalidate 패턴에 의해 한참 뒤에 방문했을 때 옛날 데이터가 먼저 보여지고, 백그라운드에서 업데이트됨. 그 다음 방문부터 새 데이터를 보게 됨. 이는 내가 최신 정보를 보기 위해 누군가가 "먼저 방문해서" 나를 위해 정보를 업데이트 해줘야하는 것으로, 적은 종류의 데이터를 다양한 사람들이 보는 뉴스, 쇼핑몰, 주가 차트 등에 더 적합함. 연구자 한 명 한 명이 자신만의 실험을 하는, **개인화된 정보가 많이 있는 앱에서는 유용한 패턴이 아님.**

| 기술                                                       | 장점                                                                                 | 단점                                                                                                                                                                                                                                                                                                                                                                                     | 결과                                                                                                                                                    |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| RSC만을 활용                                               | 매우 빠른 로딩 속도                                                                  | ISR을 적극적으로 활용한다고 해도, 데이터 업데이트가 정기적으로 일어나는게 아니라 Spike치는 경우가 잦음 (하루의 대부분의 업데이트가 1시간 내에 몰려서 일어남.) 그래서 ISR등을 사용한다고 해도 데이터가 항상 라이브 데이터이기 어려움. PPR 같은 것을 통해서 빠른 첫 페인트와 라이브 데이터를 할 수도 있지만, 결과적으로 네덜란드 왕복에 걸리는 시간 때문에 보이는 데이터는 오래 걸릴 것임. | 기각. 어떤 형식으로든 데이터를 캐싱해야함.                                                                                                              |
| Streaming SSR과 Next Cache Directives를 이용한 캐시 레이어 | 매우 빠른 로딩 속도, 하지만 클라이언트 번들은 커짐                                   | 캐시에 2MB 제한이 걸려있다는 점을 제외하고는 좋음                                                                                                                                                                                                                                                                                                                                        | 새로운 방문자가 자주 발생하는 경우가 아니기 때문에 큰 문제는 없음. 온 사람이 또 오는 경우는 클라이언트 번들이 캐시되어 있기에 그렇게 심각한 문제가 아님 |
| SWR만을 사용                                               | 데이터가 계속 최신 데이터로 유지. 실험 결과가 업데이트되면 자동으로 새 결과가 보여짐 | 최초 로딩이 너무 오래 걸림                                                                                                                                                                                                                                                                                                                                                               | SWR 데이터를                                                                                                                                            |

결과적으로 나는 RSC의 속도와 SWR의 다이나믹함을 모두 가지고 싶은 것을 찾아서 한참을 헤맸다. 당시에는 이런 클라이언트와 서버의 데이터 페칭 패턴에 대한 compositing pattern의 논의가 극초기인 상태였기 때문에, 이런 논의의 선두를 달리는 사람들과 많은 논의를 두고 받았다

대충 논의하는 캡쳐

결과적으로 당시의 편법적인 방식으로 SWR의 fallback 데이터에 서버에서 온 씨드 데이터를 넣어주고, SWR의 초기 isLoading 값을 의도적으로 꺼주는 방식을 택했다 [상세설명] 현재는 여기서 더 발전한 패턴들이 존재하는데, 가장 대표적으로 Server에서 시작된 데이터로딩 프리페치 Promise를 클라이언트로 내려준 뒤 `use` 훅을 이용해 consume하는 방식이 있다.

사실 나는 그래서 Streaming SSR보다도 오히려 fetch cache가 더 좋은듯 했다. 2MB 제한이 있어서 일부 페이지는 여전히 백그라운드에서 몰래 로딩하는 패턴이 필요하지만 말이다.

## Nested Layout과 로딩

꽤 좋았는데, 네모네모한 레이아웃이 아니면 구현하기가 복잡한 것이 아쉬웠다.

## 상태관리

SWR을 사용하게 되면서 내가 줄곧 주창했던 것은 상태 관리를 따로 하지 않아도 된다는 것이다. 어차피 모든 정보는 서버에서 관리되어야 하고, 서버 데이터를 직접 업데이트하고 그 데이터가 실시간으로 클라이언트에 SWR로 업데이트되면서 상태가 최신으로 유지될 것이기 때문에 굳이 상태 관리를 할 필요가 없다는 것이다. 흔히들 이런 말을 하면 “복잡한 앱을 만든 경험이 없어서 그렇다”고 논의가 일축되는 경향이 있고, 복잡한 앱에서 왜 그게 안 되는지 물어보아도 설득될만한 합당한 근거가 없었다.

나 또한 결국 이는 엄청난 착오임을 알게 되었다. 생각해보면 프론트엔드 복잡성의 모든 문제는 API 통제권의 부재에서 오는데, 우리의 경우도 마찬가지였다. SWR은 하나의 엔드포인트에서 오는 상태를 최신으로 유지하고 관리하는데 특화되어 있는데, 여러가지 데이터가 조합되어서 상태를 표시하는 그래프의 경우 여러 가지 SWR을 훅을 청기들어 백기들어 하는 거대한 nested된 SWR 훅을 만들어야 했다. 이를 나중에 Custom Fetch 함수를 만들어 확연한 간소화를 하긴 했지만, 결과적으로 여러 API들에서 오는 정보들을 조합해서 관리해야할 경우는 SWR 자체 만으로는 상태관리가 어렵다는 것이 결론이었다. 모든 기능에 정형화된 api가 있을 때만 할 수 있던 이상적인 그림이었다.

고칠게 많다 디펜던시가 많다 오히려 점점 더 많아짐

그렇다고 슈퍼 훅을 만들 수는 없고...

관리 포인트가 늘어난 것은 아쉽다

철저하게 명세가 안 된 것

또한 서버에 저장되지 않는 정보들도 있다는 것이 문제였다. 예를 들어 사용자들이 설정해둔 그래프의 값이나 테이블 뷰 설정 등은 나는 처음부터 URL에 기록되어야 한다고 생각했고, 그 URL을 사용자들이 북마크해서 사용해야한다고 생각했는데, 실제로 그런 유저는 많지 않았다. 결국 Local Storage의 Persistence와 URL State를 동기화해주는 코드를 작성할 수 밖에 없었다. 이는 URL을 로컬 상태의 원천으로 잡고 싶어하는 내 상황에서는 최선 같기도 했다. 허나, 테이블 정보 등의 데이터는 useLocalStorage 등을 사용해야 했다.

물론 나는 여전히 상태관리 라이브러리를 도입하는 것보다 하나의 View를 위한 하나의 API를 백엔드에서 구성해주거나, 최소한 여러 BE API를 조합해서 정리해주는 BFF를 구성해줘야 한다고 생각한다. 다만, 여러 현실적인 어려움으로, 다시 기회가 주어진다면 일부 정보들은 Local Storage Persistence를 사용하는 Zustand를 활용할 것 같다.

## 오픈소스 기여

이제는 당연하지만 하다가 막히는 상황이 생기면 업스트림 코드에 기여하는건 일상이 됐다.

배움이 있는 경험이었다.

테이블에서 액션 버튼 어질어질 새로고침됨

## 현대적인 웹을 향해

랩 사이드바에 새로 들어갈 수 있는 여지가 많아졌다.

인피니트 스크롤

확장성은 플러스

키보드 접근성

우클릭 등

멀티 액션 등

## 탠스택에 대하여

텐스택은 괜찮았는데, 테이블 등이 라이브 업데이트를 상정한 것 같지 않았다. Row 값들이 변경됐을 때 커스텀 로직을 직접 작성해줘야한다.

## 드래그앤 드랍과 리액트 포탈

이건 문제가 발생했고, 여전히 있다.

## 전반적인 후기

몇 가지 아쉬운 점도 있었는데,

데이터를 못 얻고 결정권도 없어서 나한테 풀 책임이 있냐고

인간의 결정력을 나는 믿지 않는다

## Automated ML에서 Autonomous ML을 향해
